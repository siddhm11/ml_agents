{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nixtlats in /opt/anaconda3/envs/agent_env/lib/python3.11/site-packages (0.5.2)\n",
      "Requirement already satisfied: httpx in /opt/anaconda3/envs/agent_env/lib/python3.11/site-packages (from nixtlats) (0.28.1)\n",
      "Requirement already satisfied: pandas in /opt/anaconda3/envs/agent_env/lib/python3.11/site-packages (from nixtlats) (2.3.0)\n",
      "Requirement already satisfied: pydantic in /opt/anaconda3/envs/agent_env/lib/python3.11/site-packages (from nixtlats) (2.11.7)\n",
      "Requirement already satisfied: requests in /opt/anaconda3/envs/agent_env/lib/python3.11/site-packages (from nixtlats) (2.32.4)\n",
      "Requirement already satisfied: tenacity in /opt/anaconda3/envs/agent_env/lib/python3.11/site-packages (from nixtlats) (9.1.2)\n",
      "Requirement already satisfied: utilsforecast>=0.1.7 in /opt/anaconda3/envs/agent_env/lib/python3.11/site-packages (from nixtlats) (0.2.12)\n",
      "Requirement already satisfied: numpy in /opt/anaconda3/envs/agent_env/lib/python3.11/site-packages (from utilsforecast>=0.1.7->nixtlats) (2.3.0)\n",
      "Requirement already satisfied: packaging in /opt/anaconda3/envs/agent_env/lib/python3.11/site-packages (from utilsforecast>=0.1.7->nixtlats) (25.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/anaconda3/envs/agent_env/lib/python3.11/site-packages (from pandas->nixtlats) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/anaconda3/envs/agent_env/lib/python3.11/site-packages (from pandas->nixtlats) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/anaconda3/envs/agent_env/lib/python3.11/site-packages (from pandas->nixtlats) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in /opt/anaconda3/envs/agent_env/lib/python3.11/site-packages (from python-dateutil>=2.8.2->pandas->nixtlats) (1.17.0)\n",
      "Requirement already satisfied: anyio in /opt/anaconda3/envs/agent_env/lib/python3.11/site-packages (from httpx->nixtlats) (4.9.0)\n",
      "Requirement already satisfied: certifi in /opt/anaconda3/envs/agent_env/lib/python3.11/site-packages (from httpx->nixtlats) (2025.6.15)\n",
      "Requirement already satisfied: httpcore==1.* in /opt/anaconda3/envs/agent_env/lib/python3.11/site-packages (from httpx->nixtlats) (1.0.9)\n",
      "Requirement already satisfied: idna in /opt/anaconda3/envs/agent_env/lib/python3.11/site-packages (from httpx->nixtlats) (3.10)\n",
      "Requirement already satisfied: h11>=0.16 in /opt/anaconda3/envs/agent_env/lib/python3.11/site-packages (from httpcore==1.*->httpx->nixtlats) (0.16.0)\n",
      "Requirement already satisfied: sniffio>=1.1 in /opt/anaconda3/envs/agent_env/lib/python3.11/site-packages (from anyio->httpx->nixtlats) (1.3.1)\n",
      "Requirement already satisfied: typing_extensions>=4.5 in /opt/anaconda3/envs/agent_env/lib/python3.11/site-packages (from anyio->httpx->nixtlats) (4.14.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /opt/anaconda3/envs/agent_env/lib/python3.11/site-packages (from pydantic->nixtlats) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in /opt/anaconda3/envs/agent_env/lib/python3.11/site-packages (from pydantic->nixtlats) (2.33.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in /opt/anaconda3/envs/agent_env/lib/python3.11/site-packages (from pydantic->nixtlats) (0.4.1)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /opt/anaconda3/envs/agent_env/lib/python3.11/site-packages (from requests->nixtlats) (3.4.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/anaconda3/envs/agent_env/lib/python3.11/site-packages (from requests->nixtlats) (2.4.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install nixtlats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0.4</th>\n",
       "      <th>Unnamed: 0.3</th>\n",
       "      <th>Unnamed: 0.2</th>\n",
       "      <th>Unnamed: 0.1</th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>BILLING_DATE</th>\n",
       "      <th>SITE</th>\n",
       "      <th>BRAND</th>\n",
       "      <th>FORMAT_DESC</th>\n",
       "      <th>MH_SEGMENT</th>\n",
       "      <th>MH_FAMILY</th>\n",
       "      <th>MH_CLASS</th>\n",
       "      <th>MH_BRICK</th>\n",
       "      <th>ARTICLE</th>\n",
       "      <th>TOTAL_BILLING_QTY</th>\n",
       "      <th>TOTAL_GROSS_MARGIN</th>\n",
       "      <th>TOTAL_MARKDOWN</th>\n",
       "      <th>TOTAL_NET_SALES</th>\n",
       "      <th>TOTAL_GROSS_SALES</th>\n",
       "      <th>TOTAL_DISCOUNT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>712337</td>\n",
       "      <td>712337</td>\n",
       "      <td>712337</td>\n",
       "      <td>712337</td>\n",
       "      <td>712337</td>\n",
       "      <td>2022-09-30</td>\n",
       "      <td>8013</td>\n",
       "      <td>04017</td>\n",
       "      <td>TRENDS</td>\n",
       "      <td>WOMENS WEAR</td>\n",
       "      <td>WESTERN WEAR</td>\n",
       "      <td>WINTER WEAR</td>\n",
       "      <td>SWEATSHIRTS</td>\n",
       "      <td>441146771010</td>\n",
       "      <td>1</td>\n",
       "      <td>78.66</td>\n",
       "      <td>0.0</td>\n",
       "      <td>334.28</td>\n",
       "      <td>499.0</td>\n",
       "      <td>164.72</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>669408</td>\n",
       "      <td>669408</td>\n",
       "      <td>669408</td>\n",
       "      <td>669408</td>\n",
       "      <td>669408</td>\n",
       "      <td>2022-09-30</td>\n",
       "      <td>8013</td>\n",
       "      <td>D525</td>\n",
       "      <td>TRENDS</td>\n",
       "      <td>WOMENS WEAR</td>\n",
       "      <td>WESTERN WEAR</td>\n",
       "      <td>CASUAL WEAR</td>\n",
       "      <td>T SHIRTS</td>\n",
       "      <td>441137882004</td>\n",
       "      <td>1</td>\n",
       "      <td>71.91</td>\n",
       "      <td>0.0</td>\n",
       "      <td>240.26</td>\n",
       "      <td>399.0</td>\n",
       "      <td>158.74</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>236195</td>\n",
       "      <td>236195</td>\n",
       "      <td>236195</td>\n",
       "      <td>236195</td>\n",
       "      <td>236195</td>\n",
       "      <td>2022-09-30</td>\n",
       "      <td>8013</td>\n",
       "      <td>09888</td>\n",
       "      <td>TRENDS</td>\n",
       "      <td>MENS CASUAL</td>\n",
       "      <td>ACTIVE WEAR</td>\n",
       "      <td>TOPS</td>\n",
       "      <td>T SHIRTS</td>\n",
       "      <td>441134761003</td>\n",
       "      <td>1</td>\n",
       "      <td>88.72</td>\n",
       "      <td>0.0</td>\n",
       "      <td>322.58</td>\n",
       "      <td>599.0</td>\n",
       "      <td>276.42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>298014</td>\n",
       "      <td>298014</td>\n",
       "      <td>298014</td>\n",
       "      <td>298014</td>\n",
       "      <td>298014</td>\n",
       "      <td>2022-09-30</td>\n",
       "      <td>8013</td>\n",
       "      <td>04063</td>\n",
       "      <td>TRENDS</td>\n",
       "      <td>MENS CASUAL</td>\n",
       "      <td>ACTIVE WEAR</td>\n",
       "      <td>TOPS</td>\n",
       "      <td>T SHIRTS</td>\n",
       "      <td>441145712004</td>\n",
       "      <td>1</td>\n",
       "      <td>408.46</td>\n",
       "      <td>0.0</td>\n",
       "      <td>799.00</td>\n",
       "      <td>799.0</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>776725</td>\n",
       "      <td>776725</td>\n",
       "      <td>776725</td>\n",
       "      <td>776725</td>\n",
       "      <td>776725</td>\n",
       "      <td>2022-09-30</td>\n",
       "      <td>8013</td>\n",
       "      <td>04042</td>\n",
       "      <td>TRENDS</td>\n",
       "      <td>MENS WEAR</td>\n",
       "      <td>SMART CASUALS</td>\n",
       "      <td>BOTTOMS</td>\n",
       "      <td>TROUSERS</td>\n",
       "      <td>441138083006</td>\n",
       "      <td>1</td>\n",
       "      <td>239.82</td>\n",
       "      <td>0.0</td>\n",
       "      <td>790.98</td>\n",
       "      <td>999.0</td>\n",
       "      <td>208.02</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0.4  Unnamed: 0.3  Unnamed: 0.2  Unnamed: 0.1  Unnamed: 0  \\\n",
       "0        712337        712337        712337        712337      712337   \n",
       "1        669408        669408        669408        669408      669408   \n",
       "2        236195        236195        236195        236195      236195   \n",
       "3        298014        298014        298014        298014      298014   \n",
       "4        776725        776725        776725        776725      776725   \n",
       "\n",
       "  BILLING_DATE  SITE  BRAND FORMAT_DESC   MH_SEGMENT      MH_FAMILY  \\\n",
       "0   2022-09-30  8013  04017      TRENDS  WOMENS WEAR   WESTERN WEAR   \n",
       "1   2022-09-30  8013   D525      TRENDS  WOMENS WEAR   WESTERN WEAR   \n",
       "2   2022-09-30  8013  09888      TRENDS  MENS CASUAL    ACTIVE WEAR   \n",
       "3   2022-09-30  8013  04063      TRENDS  MENS CASUAL    ACTIVE WEAR   \n",
       "4   2022-09-30  8013  04042      TRENDS    MENS WEAR  SMART CASUALS   \n",
       "\n",
       "      MH_CLASS     MH_BRICK       ARTICLE  TOTAL_BILLING_QTY  \\\n",
       "0  WINTER WEAR  SWEATSHIRTS  441146771010                  1   \n",
       "1  CASUAL WEAR     T SHIRTS  441137882004                  1   \n",
       "2         TOPS     T SHIRTS  441134761003                  1   \n",
       "3         TOPS     T SHIRTS  441145712004                  1   \n",
       "4      BOTTOMS     TROUSERS  441138083006                  1   \n",
       "\n",
       "   TOTAL_GROSS_MARGIN  TOTAL_MARKDOWN  TOTAL_NET_SALES  TOTAL_GROSS_SALES  \\\n",
       "0               78.66             0.0           334.28              499.0   \n",
       "1               71.91             0.0           240.26              399.0   \n",
       "2               88.72             0.0           322.58              599.0   \n",
       "3              408.46             0.0           799.00              799.0   \n",
       "4              239.82             0.0           790.98              999.0   \n",
       "\n",
       "   TOTAL_DISCOUNT  \n",
       "0          164.72  \n",
       "1          158.74  \n",
       "2          276.42  \n",
       "3            0.00  \n",
       "4          208.02  "
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"data-8013-trends.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['BILLING_DATE'] = pd.to_datetime(df['BILLING_DATE'])\n",
    "\n",
    "df.to_csv( \"data-8013-trends.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from io import StringIO\n",
    "\n",
    "# Load your data (assuming it's in CSV format)\n",
    "\n",
    "# Sort by BILLING_DATE\n",
    "df_sorted = df.sort_values(by='BILLING_DATE')\n",
    "df_sorted.to_csv( \"data-8013-trends.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(823225, 18)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The error occurs because 'shape' is not a standalone function; it is an attribute of DataFrame objects.\n",
    "# To get the shape of df_sorted, use:\n",
    "df_sorted.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 STARTING FIXED TIMEGPT PIPELINE WITH EXOGENOUS FEATURES\n",
      "======================================================================\n",
      "\n",
      "📋 STEP 1: DATA CLEANING WITH EXOGENOUS FEATURES\n",
      "   • Creating exogenous features before data aggregation...\n",
      "   • Created 11 predictable exogenous features\n",
      "   • Found 12853 negative sales records\n",
      "   📊 Product filtering with exogenous features:\n",
      "      • Viable for forecasting: 381\n",
      "      • Selected top: 8\n",
      "   • Aggregating with enhanced exogenous variables...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:nixtlats.nixtla_client:Validating inputs...\n",
      "INFO:nixtlats.nixtla_client:Preprocessing dataframes...\n",
      "WARNING:nixtlats.nixtla_client:You did not provide X_df. Exogenous variables in df are ignored. To surpress this warning, please add X_df with exogenous variables: year, month, day_of_week, is_weekend, is_month_start, is_month_end, is_quarter_start, is_quarter_end, is_republic_day, is_holi, is_diwali\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ✅ Enhanced data with exogenous features: 6,245 rows, 8 products\n",
      "   📊 Exogenous features included: 11\n",
      "\n",
      "🔧 STEP 2: FIXING FREQUENCY ISSUES\n",
      "   • Applying robust frequency correction for irregular data...\n",
      "      Processing: 04042 | MENS WEAR | SMART CASUALS | BOTT... (790 records)\n",
      "        📊 Missing ratio: 0.25%\n",
      "        ✅ Success: 2 gaps filled\n",
      "      Processing: 04042 | MENS WEAR | SMART CASUALS | TOPS... (792 records)\n",
      "        📊 Missing ratio: 0.00%\n",
      "        ✅ Success: 0 gaps filled\n",
      "      Processing: 04063 | MENS CASUAL | ACTIVE WEAR | BOTT... (791 records)\n",
      "        📊 Missing ratio: 0.13%\n",
      "        ✅ Success: 1 gaps filled\n",
      "      Processing: 04063 | MENS CASUAL | ACTIVE WEAR | TOPS... (792 records)\n",
      "        📊 Missing ratio: 0.00%\n",
      "        ✅ Success: 0 gaps filled\n",
      "      Processing: D699 | WOMENS WEAR | ETHNIC WEAR | TOPWE... (788 records)\n",
      "        📊 Missing ratio: 0.51%\n",
      "        ✅ Success: 4 gaps filled\n",
      "      Processing: D699 | WOMENS WEAR | ETHNIC WEAR | TOPWE... (709 records)\n",
      "        📊 Missing ratio: 10.37%\n",
      "        ✅ Success: 82 gaps filled\n",
      "      Processing: F085 | WOMENS WEAR | ETHNIC WEAR | TOPWE... (792 records)\n",
      "        📊 Missing ratio: 0.00%\n",
      "        ✅ Success: 0 gaps filled\n",
      "      Processing: F639 | WOMENS WEAR | ETHNIC WEAR | TOPWE... (791 records)\n",
      "        📊 Missing ratio: 0.13%\n",
      "        ✅ Success: 1 gaps filled\n",
      "   ✅ Robust processing complete: 8 products\n",
      "   📊 Final dataset: 6,335 rows\n",
      "\n",
      "✂️ STEP 3: TRAIN-TEST SPLIT\n",
      "   • Splitting data with 21 test days...\n",
      "   ✅ Train: 6167 rows, Test: 168 rows\n",
      "\n",
      "🤖 ST`EP 4: TIMEGPT FORECASTING WITH EXOGENOUS FEATURES - FIXED\n",
      "   🎯 Running FIXED TimeGPT forecast for 7 periods...\n",
      "   • Using exogenous variables: ['year', 'month', 'day_of_week', 'is_weekend', 'is_month_start', 'is_month_end', 'is_quarter_start', 'is_quarter_end', 'is_republic_day', 'is_holi', 'is_diwali']\n",
      "   • Forecast data shape: (6167, 14)\n",
      "   • Columns: ['unique_id', 'ds', 'y', 'year', 'month', 'day_of_week', 'is_weekend', 'is_month_start', 'is_month_end', 'is_quarter_start', 'is_quarter_end', 'is_republic_day', 'is_holi', 'is_diwali']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:nixtlats.nixtla_client:Calling Forecast Endpoint...\n",
      "INFO:nixtlats.nixtla_client:Validating inputs...\n",
      "INFO:nixtlats.nixtla_client:Preprocessing dataframes...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ✅ FIXED forecasts generated: 56 rows\n",
      "\n",
      "📊 STEP 5: MODEL EVALUATION\n",
      "   • Calculating performance metrics...\n",
      "   📊 Data quality check:\n",
      "      • Zero values in actual data: 0\n",
      "      • Near-zero values: 0\n",
      "   ✅ Evaluation complete: 8 products evaluated\n",
      "   📊 Average MAE: 6776.82\n",
      "   📊 Average RMSE: 8344.29\n",
      "   📊 Average sMAPE: 36.98% (recommended)\n",
      "   📊 Average MAPE: 41.46% (calculated from 8/8 products)\n",
      "   ⚠️  Products with zero values: 0\n",
      "\n",
      "📈 STEP 6: CREATING VISUALIZATIONS\n",
      "   • Creating TimeGPT visualization...\n",
      "   ✅ TimeGPT native plot created\n",
      "\n",
      "🔮 STEP 7: FUTURE FORECASTING - FIXED\n",
      "   • Generating 14-day future forecasts...\n",
      "   • Including exogenous features: ['year', 'month', 'day_of_week', 'is_weekend', 'is_month_start', 'is_month_end', 'is_quarter_start', 'is_quarter_end', 'is_republic_day', 'is_holi', 'is_diwali']\n",
      "   • Historical data shape: (6335, 14)\n",
      "   • Future exogenous data shape: (112, 13)\n",
      "   ❌ Future forecast error: Some of your exogenous variables contain NA, please check\n"
     ]
    }
   ],
   "source": [
    "# Import required libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "from nixtlats import TimeGPT\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Initialize TimeGPT client\n",
    "timegpt = TimeGPT(token='nixak-AI31YupjpWhin07kPGKOhvW5zj8IliWOlTSWLEInpOuKHNWNhtSETsXIFgyEiYT58g3Hk0hvMFhnJpdS')  # Replace with your actual token\n",
    "\n",
    "def complete_timegpt_pipeline_with_exogenous(df_sorted, top_n_products=8):\n",
    "    \"\"\"\n",
    "    Complete end-to-end TimeGPT forecasting pipeline with exogenous features\n",
    "    \"\"\"\n",
    "    print(\"🚀 STARTING ENHANCED TIMEGPT PIPELINE WITH EXOGENOUS FEATURES\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    # Step 1: Enhanced Data Cleaning and Preparation with Exogenous Features\n",
    "    print(\"\\n📋 STEP 1: DATA CLEANING WITH EXOGENOUS FEATURES\")\n",
    "    cleaned_data, predictable_features = clean_and_prepare_data_with_exogenous(df_sorted, top_n_products)\n",
    "    \n",
    "    # Step 2: Fix Frequency Issues\n",
    "    print(\"\\n🔧 STEP 2: FIXING FREQUENCY ISSUES\")\n",
    "    frequency_fixed_data = fix_frequency_issues_complete(cleaned_data)\n",
    "    \n",
    "    # Step 3: Train-Test Split\n",
    "    print(\"\\n✂️ STEP 3: TRAIN-TEST SPLIT\")\n",
    "    train_data, test_data = create_train_test_split(frequency_fixed_data)\n",
    "    \n",
    "    # Step 4: TimeGPT Forecasting with Exogenous Features\n",
    "    print(\"\\n🤖 STEP 4: TIMEGPT FORECASTING WITH EXOGENOUS FEATURES\")\n",
    "    forecasts = run_timegpt_forecasting_fixed(train_data, predictable_features, test_periods=7)\n",
    "    \n",
    "    # Step 5: Evaluation\n",
    "    print(\"\\n📊 STEP 5: MODEL EVALUATION\")\n",
    "    evaluation_results = evaluate_forecasts(forecasts, test_data)\n",
    "    \n",
    "    # Step 6: Visualization\n",
    "    print(\"\\n📈 STEP 6: CREATING VISUALIZATIONS\")\n",
    "    create_comprehensive_visualizations(frequency_fixed_data, forecasts, test_data, evaluation_results)\n",
    "    \n",
    "    # Step 7: Future Forecasting\n",
    "    print(\"\\n🔮 STEP 7: FUTURE FORECASTING\")\n",
    "    future_forecasts = generate_future_forecasts(frequency_fixed_data, horizon=14)  # Reduced horizon\n",
    "    \n",
    "    return {\n",
    "        'cleaned_data': frequency_fixed_data,\n",
    "        'train_data': train_data,\n",
    "        'test_data': test_data,\n",
    "        'forecasts': forecasts,\n",
    "        'evaluation': evaluation_results,\n",
    "        'future_forecasts': future_forecasts,\n",
    "        'exogenous_features': predictable_features\n",
    "    }\n",
    "\n",
    "def clean_and_prepare_data_with_exogenous(df_sorted, top_n_products=8):\n",
    "    \"\"\"\n",
    "    Enhanced data cleaning with exogenous feature integration\n",
    "    \"\"\"\n",
    "    print(\"   • Creating exogenous features before data aggregation...\")\n",
    "    \n",
    "    # ✅ Step 1: Create exogenous features on raw data FIRST\n",
    "    df_enhanced, predictable_features = prepare_exogenous_data_for_timegpt(df_sorted)\n",
    "    \n",
    "    print(f\"   • Created {len(predictable_features)} predictable exogenous features\")\n",
    "    \n",
    "    # Handle negative sales values\n",
    "    print(f\"   • Found {(df_enhanced['TOTAL_NET_SALES'] < 0).sum()} negative sales records\")\n",
    "    df_enhanced.loc[df_enhanced['TOTAL_NET_SALES'] < 0, 'TOTAL_NET_SALES'] = 0\n",
    "    \n",
    "    # Create PRODUCT_KEY if not exists\n",
    "    if 'PRODUCT_KEY' not in df_enhanced.columns:\n",
    "        df_enhanced['PRODUCT_KEY'] = (\n",
    "            df_enhanced['BRAND'].astype(str) + ' | ' + \n",
    "            df_enhanced['MH_SEGMENT'].astype(str) + ' | ' + \n",
    "            df_enhanced['MH_FAMILY'].astype(str) + ' | ' + \n",
    "            df_enhanced['MH_CLASS'].astype(str) + ' | ' + \n",
    "            df_enhanced['MH_BRICK'].astype(str)\n",
    "        )\n",
    "    \n",
    "    # Product filtering (same as before)\n",
    "    product_analysis = df_enhanced.groupby('PRODUCT_KEY').agg({\n",
    "        'TOTAL_NET_SALES': ['sum', 'mean', 'count', 'std'],\n",
    "        'BILLING_DATE': ['min', 'max', 'nunique'],\n",
    "        'TOTAL_BILLING_QTY': 'sum',\n",
    "        'TOTAL_GROSS_MARGIN': 'sum'\n",
    "    }).round(2)\n",
    "    \n",
    "    product_analysis.columns = ['total_sales', 'avg_sales', 'transaction_count','sales_std', 'first_date', 'last_date', 'unique_days','total_qty', 'total_margin']\n",
    "    \n",
    "    # Calculate quality metrics\n",
    "    product_analysis['data_span_days'] = (product_analysis['last_date'] - product_analysis['first_date']).dt.days + 1\n",
    "    product_analysis['data_density'] = product_analysis['unique_days'] / product_analysis['data_span_days']\n",
    "    product_analysis['cv'] = product_analysis['sales_std'] / product_analysis['avg_sales']\n",
    "    \n",
    "    # Product filtering\n",
    "    viable_products = product_analysis[\n",
    "        (product_analysis['unique_days'] >= 60) &\n",
    "        (product_analysis['data_span_days'] >= 90) &\n",
    "        (product_analysis['data_density'] >= 0.3) &\n",
    "        (product_analysis['total_sales'] >= 10000) &\n",
    "        (product_analysis['transaction_count'] >= 30) &\n",
    "        (product_analysis['cv'] < 3)\n",
    "    ].sort_values('total_sales', ascending=False)\n",
    "    \n",
    "    print(f\"   📊 Product filtering with exogenous features:\")\n",
    "    print(f\"      • Viable for forecasting: {len(viable_products)}\")\n",
    "    print(f\"      • Selected top: {min(top_n_products, len(viable_products))}\")\n",
    "    \n",
    "    if len(viable_products) == 0:\n",
    "        viable_products = product_analysis[\n",
    "            (product_analysis['unique_days'] >= 30) &\n",
    "            (product_analysis['total_sales'] >= 5000)\n",
    "        ].sort_values('total_sales', ascending=False)\n",
    "    \n",
    "    selected_products = viable_products.head(min(top_n_products, len(viable_products))).index.tolist()\n",
    "    filtered_df = df_enhanced[df_enhanced['PRODUCT_KEY'].isin(selected_products)].copy()\n",
    "    \n",
    "    print(\"   • Aggregating with enhanced exogenous variables...\")\n",
    "    \n",
    "    # ✅ Step 2: Aggregate including exogenous features\n",
    "    agg_dict = {\n",
    "        'TOTAL_NET_SALES': 'sum',\n",
    "        'TOTAL_BILLING_QTY': 'sum',\n",
    "        'TOTAL_GROSS_MARGIN': 'sum',\n",
    "        'TOTAL_GROSS_SALES': 'sum',\n",
    "        'TOTAL_DISCOUNT': 'sum'\n",
    "    }\n",
    "    \n",
    "    # Add predictable exogenous features to aggregation (using 'first' since they're constant per day)\n",
    "    for feature in predictable_features:\n",
    "        if feature in filtered_df.columns:\n",
    "            agg_dict[feature] = 'first'\n",
    "    \n",
    "    daily_sales = filtered_df.groupby(['BILLING_DATE', 'PRODUCT_KEY']).agg(agg_dict).reset_index()\n",
    "    \n",
    "    # Rename columns for TimeGPT\n",
    "    column_mapping = {\n",
    "        'BILLING_DATE': 'ds',\n",
    "        'TOTAL_NET_SALES': 'y',\n",
    "        'PRODUCT_KEY': 'unique_id',\n",
    "        'TOTAL_BILLING_QTY': 'qty',\n",
    "        'TOTAL_GROSS_MARGIN': 'margin',\n",
    "        'TOTAL_GROSS_SALES': 'gross_sales',\n",
    "        'TOTAL_DISCOUNT': 'discount'\n",
    "    }\n",
    "    \n",
    "    timegpt_data = daily_sales.rename(columns=column_mapping)\n",
    "    \n",
    "    # Data type optimization\n",
    "    timegpt_data['ds'] = pd.to_datetime(timegpt_data['ds'])\n",
    "    for col in ['y', 'qty', 'margin', 'gross_sales', 'discount']:\n",
    "        if col in timegpt_data.columns:\n",
    "            timegpt_data[col] = pd.to_numeric(timegpt_data[col], errors='coerce')\n",
    "    \n",
    "    # Remove invalid records\n",
    "    timegpt_data = timegpt_data.dropna(subset=['y'])\n",
    "    \n",
    "    print(f\"   ✅ Enhanced data with exogenous features: {len(timegpt_data):,} rows, {timegpt_data['unique_id'].nunique()} products\")\n",
    "    print(f\"   📊 Exogenous features included: {len([f for f in predictable_features if f in timegpt_data.columns])}\")\n",
    "    \n",
    "    return timegpt_data, predictable_features\n",
    "\n",
    "\n",
    "def create_calendar_features(df):\n",
    "    \"\"\"Create comprehensive calendar features for Indian retail\"\"\"\n",
    "    df = df.copy()\n",
    "    df['BILLING_DATE'] = pd.to_datetime(df['BILLING_DATE'])\n",
    "    \n",
    "    # Basic temporal features\n",
    "    df['year'] = df['BILLING_DATE'].dt.year\n",
    "    df['month'] = df['BILLING_DATE'].dt.month\n",
    "    df['day_of_week'] = df['BILLING_DATE'].dt.dayofweek\n",
    "    df['day_of_year'] = df['BILLING_DATE'].dt.dayofyear\n",
    "    df['week_of_year'] = df['BILLING_DATE'].dt.isocalendar().week\n",
    "    df['quarter'] = df['BILLING_DATE'].dt.quarter\n",
    "    \n",
    "    # Weekend indicator\n",
    "    df['is_weekend'] = (df['day_of_week'] >= 5).astype(int)\n",
    "    \n",
    "    # Month-end/start effects\n",
    "    df['is_month_start'] = df['BILLING_DATE'].dt.is_month_start.astype(int)\n",
    "    df['is_month_end'] = df['BILLING_DATE'].dt.is_month_end.astype(int)\n",
    "    df['is_quarter_start'] = df['BILLING_DATE'].dt.is_quarter_start.astype(int)\n",
    "    df['is_quarter_end'] = df['BILLING_DATE'].dt.is_quarter_end.astype(int)\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "def create_indian_holiday_features(df):\n",
    "    \"\"\"Add Indian holiday indicators based on 2024 calendar\"\"\"\n",
    "    df = df.copy()\n",
    "    \n",
    "    # Major Indian holidays for 2024 (extend for other years)\n",
    "    major_holidays = {\n",
    "        '2024-01-26': 'republic_day',\n",
    "        '2024-03-25': 'holi', \n",
    "        '2024-04-11': 'eid_ul_fitr',\n",
    "        '2024-04-17': 'ram_navami',\n",
    "        '2024-08-15': 'independence_day',\n",
    "        '2024-08-26': 'janmashtami',\n",
    "        '2024-10-12': 'dussehra',\n",
    "        '2024-10-31': 'diwali',\n",
    "        '2024-12-25': 'christmas'\n",
    "    }\n",
    "    \n",
    "    # Create holiday indicators\n",
    "    for date_str, holiday_name in major_holidays.items():\n",
    "        df[f'is_{holiday_name}'] = (df['BILLING_DATE'].dt.strftime('%Y-%m-%d') == date_str).astype(int)\n",
    "    \n",
    "    # ✅ FIXED: Holiday proximity features (corrected variable names)\n",
    "    diwali_date = pd.to_datetime('2024-10-31')\n",
    "    for days in [1, 2, 3]:\n",
    "        df[f'days_before_diwali_{days}'] = (df['BILLING_DATE'] == diwali_date - pd.Timedelta(days=days)).astype(int)\n",
    "        df[f'days_after_diwali_{days}'] = (df['BILLING_DATE'] == diwali_date + pd.Timedelta(days=days)).astype(int)\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def create_business_features(df):\n",
    "    \"\"\"Create features from existing business data\"\"\"\n",
    "    df = df.copy()\n",
    "    \n",
    "    # Categorical encoding for key business dimensions\n",
    "    categorical_cols = ['SITE', 'BRAND', 'FORMAT_DESC', 'MH_SEGMENT', 'MH_FAMILY']\n",
    "    \n",
    "    for col in categorical_cols:\n",
    "        # Create frequency encoding\n",
    "        freq_map = df[col].value_counts(normalize=True).to_dict()\n",
    "        df[f'{col}_frequency'] = df[col].map(freq_map)\n",
    "        \n",
    "        # One-hot encode top categories only (to avoid high dimensionality)\n",
    "        top_categories = df[col].value_counts().head(5).index\n",
    "        for category in top_categories:\n",
    "            df[f'{col}_{category}'] = (df[col] == category).astype(int)\n",
    "    \n",
    "    # Price-related features\n",
    "    df['avg_selling_price'] = df['TOTAL_NET_SALES'] / df['TOTAL_BILLING_QTY'].replace(0, 1)\n",
    "    df['discount_rate'] = df['TOTAL_DISCOUNT'] / df['TOTAL_GROSS_SALES'].replace(0, 1)\n",
    "    df['margin_rate'] = df['TOTAL_GROSS_MARGIN'] / df['TOTAL_NET_SALES'].replace(0, 1)\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "def prepare_exogenous_data_for_timegpt(df, forecast_horizon=14):\n",
    "    \"\"\"Prepare exogenous data in TimeGPT format\"\"\"\n",
    "    \n",
    "    # Step 1: Create all features\n",
    "    df_enhanced = create_calendar_features(df)\n",
    "    df_enhanced = create_indian_holiday_features(df_enhanced)\n",
    "    df_enhanced = create_business_features(df_enhanced)\n",
    "    \n",
    "    # Step 2: Separate predictable vs unpredictable features\n",
    "    predictable_features = [\n",
    "        'year', 'month', 'day_of_week', 'is_weekend', 'is_month_start', \n",
    "        'is_month_end', 'is_quarter_start', 'is_quarter_end',\n",
    "        'is_republic_day', 'is_holi', 'is_diwali'  # Add all holiday features\n",
    "    ]\n",
    "        \n",
    "    return df_enhanced, predictable_features\n",
    "\n",
    "\n",
    "\n",
    "def fix_frequency_issues_complete(data):\n",
    "    \"\"\"\n",
    "    Robust frequency fixing optimized for your irregular data patterns\n",
    "    \"\"\"\n",
    "    print(\"   • Applying robust frequency correction for irregular data...\")\n",
    "    \n",
    "    cleaned_data = []\n",
    "    successful_products = []\n",
    "    \n",
    "    for product_id, group in data.groupby('unique_id'):\n",
    "        # Sort and remove duplicates\n",
    "        group = group.sort_values('ds').drop_duplicates(subset=['ds'], keep='first')\n",
    "        \n",
    "        print(f\"      Processing: {product_id[:40]}... ({len(group)} records)\")\n",
    "        \n",
    "        # Skip products with insufficient data (stricter threshold)\n",
    "        if len(group) < 50:  # Increased from 30 to 50\n",
    "            print(f\"        ❌ Insufficient data: {len(group)} < 50 days\")\n",
    "            continue\n",
    "        \n",
    "        # Analyze existing frequency pattern\n",
    "        date_diffs = group['ds'].diff().dropna()\n",
    "        \n",
    "        # Create complete date range\n",
    "        date_range = pd.date_range(\n",
    "            start=group['ds'].min(),\n",
    "            end=group['ds'].max(),\n",
    "            freq='D'\n",
    "        )\n",
    "        \n",
    "        expected_points = len(date_range)\n",
    "        missing_points = expected_points - len(group)\n",
    "        missing_ratio = missing_points / expected_points\n",
    "        \n",
    "        print(f\"        📊 Missing ratio: {missing_ratio:.2%}\")\n",
    "        \n",
    "        # Skip products with too much missing data\n",
    "        if missing_ratio > 0.7:  # More than 70% missing\n",
    "            print(f\"        ❌ Too sparse: {missing_ratio:.1%} missing data\")\n",
    "            continue\n",
    "        \n",
    "        # Create complete time series\n",
    "        complete_ts = pd.DataFrame({\n",
    "            'ds': date_range,\n",
    "            'unique_id': product_id\n",
    "        })\n",
    "        \n",
    "        # Merge with actual data\n",
    "        merged = pd.merge(complete_ts, group, on=['ds', 'unique_id'], how='left')\n",
    "        \n",
    "        # Enhanced interpolation strategy based on missing ratio\n",
    "        numeric_cols = ['y', 'qty', 'margin', 'gross_sales', 'discount']\n",
    "        \n",
    "        for col in numeric_cols:\n",
    "            if col in merged.columns:\n",
    "                if missing_ratio < 0.2:  # Less than 20% missing - linear interpolation\n",
    "                    merged[col] = merged[col].interpolate(method='linear')\n",
    "                elif missing_ratio < 0.4:  # 20-40% missing - limit interpolation span\n",
    "                    merged[col] = merged[col].interpolate(method='linear', limit=5)\n",
    "                else:  # 40-70% missing - very conservative interpolation\n",
    "                    merged[col] = merged[col].interpolate(method='linear', limit=2)\n",
    "                \n",
    "                # Forward/backward fill for remaining gaps, then zero\n",
    "                merged[col] = merged[col].fillna(method='ffill', limit=3)\n",
    "                merged[col] = merged[col].fillna(method='bfill', limit=3)\n",
    "                merged[col] = merged[col].fillna(0)\n",
    "                merged[col] = merged[col].clip(lower=0)  # Ensure non-negative\n",
    "        \n",
    "        # Verify frequency can be inferred\n",
    "        freq_check = pd.infer_freq(merged['ds'])\n",
    "        if freq_check == 'D':\n",
    "            cleaned_data.append(merged)\n",
    "            successful_products.append(product_id)\n",
    "            print(f\"        ✅ Success: {missing_points} gaps filled\")\n",
    "        else:\n",
    "            print(f\"        ❌ Frequency check failed\")\n",
    "    \n",
    "    if cleaned_data:\n",
    "        final_data = pd.concat(cleaned_data, ignore_index=True)\n",
    "        print(f\"   ✅ Robust processing complete: {len(successful_products)} products\")\n",
    "        print(f\"   📊 Final dataset: {len(final_data):,} rows\")\n",
    "        return final_data\n",
    "    else:\n",
    "        raise ValueError(\"No products survived robust frequency correction\")\n",
    "\n",
    "def create_train_test_split(data, test_days=21):\n",
    "    \"\"\"\n",
    "    Create train-test split\n",
    "    \"\"\"\n",
    "    print(f\"   • Splitting data with {test_days} test days...\")\n",
    "    \n",
    "    # Split data by taking last N days for testing\n",
    "    test_data = data.groupby(\"unique_id\").tail(test_days)\n",
    "    train_data = (data.groupby(\"unique_id\").apply(lambda group: group.iloc[:-test_days]).reset_index(drop=True))\n",
    "    \n",
    "    print(f\"   ✅ Train: {len(train_data)} rows, Test: {len(test_data)} rows\")\n",
    "    \n",
    "    return train_data, test_data\n",
    "\n",
    "def run_timegpt_forecasting_fixed(train_data, predictable_features, test_periods=7):\n",
    "    \"\"\"\n",
    "    CORRECTED TimeGPT forecasting with proper exogenous variable format\n",
    "    \"\"\"\n",
    "    print(f\"   🎯 Running CORRECTED TimeGPT forecast for {test_periods} periods...\")\n",
    "    \n",
    "    try:\n",
    "        # ✅ CORRECT APPROACH: Separate main dataframe and exogenous variables\n",
    "        available_exog = [f for f in predictable_features if f in train_data.columns]\n",
    "        \n",
    "        if available_exog:\n",
    "            print(f\"   • Separating exogenous variables: {available_exog}\")\n",
    "            \n",
    "            # Main dataframe with only target variable\n",
    "            main_data = train_data[['unique_id', 'ds', 'y']].copy()\n",
    "            \n",
    "            # Separate exogenous dataframe\n",
    "            X_df = train_data[['unique_id', 'ds'] + available_exog].copy()\n",
    "            \n",
    "            print(f\"   • Main data shape: {main_data.shape}\")\n",
    "            print(f\"   • Exogenous data shape: {X_df.shape}\")\n",
    "            print(f\"   • Exogenous columns: {available_exog}\")\n",
    "            \n",
    "            # Generate forecasts with separate dataframes\n",
    "            forecasts = timegpt.forecast(\n",
    "                df=main_data,          # ✅ Main dataframe with target only\n",
    "                X_df=X_df,            # ✅ Separate exogenous dataframe\n",
    "                h=test_periods,\n",
    "                freq='D',\n",
    "                level=[80, 90],\n",
    "                time_col='ds',\n",
    "                target_col='y',\n",
    "                id_col='unique_id',\n",
    "                # Fine-tuning parameters\n",
    "                finetune_steps=30,\n",
    "                finetune_loss='mae',\n",
    "                clean_ex_first=True\n",
    "            )\n",
    "            \n",
    "        else:\n",
    "            print(\"   • No exogenous variables available, using target only\")\n",
    "            main_data = train_data[['unique_id', 'ds', 'y']].copy()\n",
    "            \n",
    "            forecasts = timegpt.forecast(\n",
    "                df=main_data,\n",
    "                h=test_periods,\n",
    "                freq='D',\n",
    "                level=[80, 90],\n",
    "                time_col='ds',\n",
    "                target_col='y',\n",
    "                id_col='unique_id',\n",
    "                finetune_steps=30,\n",
    "                finetune_loss='mae'\n",
    "            )\n",
    "        \n",
    "        print(f\"   ✅ CORRECTED forecasts generated: {len(forecasts):,} rows\")\n",
    "        return forecasts\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"   ❌ Corrected forecasting failed: {e}\")\n",
    "        print(\"   🔄 Trying basic approach without exogenous variables...\")\n",
    "        \n",
    "        # Fallback: Use only target variables\n",
    "        try:\n",
    "            basic_data = train_data[['unique_id', 'ds', 'y']].copy()\n",
    "            \n",
    "            forecasts = timegpt.forecast(\n",
    "                df=basic_data,\n",
    "                h=test_periods,\n",
    "                freq='D',\n",
    "                level=[80, 90],\n",
    "                time_col='ds',\n",
    "                target_col='y',\n",
    "                id_col='unique_id',\n",
    "                finetune_steps=20,\n",
    "                finetune_loss='mae'\n",
    "            )\n",
    "            \n",
    "            print(f\"   ✅ Basic forecasts generated: {len(forecasts):,} rows\")\n",
    "            return forecasts\n",
    "            \n",
    "        except Exception as e2:\n",
    "            print(f\"   ❌ All approaches failed: {e2}\")\n",
    "            \n",
    "            # Final fallback - minimal configuration\n",
    "            minimal_data = train_data[['unique_id', 'ds', 'y']].head(1000)  # Reduce data size\n",
    "            \n",
    "            forecasts = timegpt.forecast(\n",
    "                df=minimal_data,\n",
    "                h=test_periods,\n",
    "                freq='D',\n",
    "                time_col='ds',\n",
    "                target_col='y',\n",
    "                id_col='unique_id'\n",
    "            )\n",
    "            \n",
    "            print(f\"   ✅ Minimal fallback forecasts: {len(forecasts):,} rows\")\n",
    "            return forecasts\n",
    "\n",
    "\n",
    "def generate_future_forecasts(data, horizon=14):\n",
    "    \"\"\"\n",
    "    Generate future forecasts with proper exogenous variable handling\n",
    "    \"\"\"\n",
    "    print(f\"   • Generating {horizon}-day future forecasts...\")\n",
    "    \n",
    "    try:\n",
    "        # Check if exogenous variables exist\n",
    "        exog_cols = ['year', 'month', 'day_of_week', 'is_weekend', 'is_month_start', \n",
    "                    'is_month_end', 'is_quarter_start', 'is_quarter_end',\n",
    "                    'is_republic_day', 'is_holi', 'is_diwali']\n",
    "        \n",
    "        available_exog = [col for col in exog_cols if col in data.columns]\n",
    "        \n",
    "        if available_exog:\n",
    "            # Create future exogenous data\n",
    "            future_X_df = create_future_exogenous_data(data, available_exog, horizon)\n",
    "            \n",
    "            # Main data for forecasting\n",
    "            main_data = data[['unique_id', 'ds', 'y']].copy()\n",
    "            \n",
    "            future_forecasts = timegpt.forecast(\n",
    "                df=main_data,\n",
    "                X_df=future_X_df,  # Future exogenous data\n",
    "                h=horizon,\n",
    "                freq='D',\n",
    "                level=[80, 90],\n",
    "                time_col='ds',\n",
    "                target_col='y',\n",
    "                id_col='unique_id'\n",
    "            )\n",
    "        else:\n",
    "            # No exogenous variables\n",
    "            future_forecasts = timegpt.forecast(\n",
    "                df=data[['unique_id', 'ds', 'y']],\n",
    "                h=horizon,\n",
    "                freq='D',\n",
    "                level=[80, 90],\n",
    "                time_col='ds',\n",
    "                target_col='y',\n",
    "                id_col='unique_id'\n",
    "            )\n",
    "        \n",
    "        # Create summary\n",
    "        forecast_summary = future_forecasts.groupby('unique_id').agg({\n",
    "            'TimeGPT': ['mean', 'sum']\n",
    "        }).round(2)\n",
    "        \n",
    "        forecast_summary.columns = ['avg_daily', 'total_forecast']\n",
    "        forecast_summary = forecast_summary.sort_values('total_forecast', ascending=False)\n",
    "        \n",
    "        print(f\"   ✅ Future forecasts generated\")\n",
    "        print(f\"   📊 Top 5 products by projected sales:\")\n",
    "        print(forecast_summary.head())\n",
    "        \n",
    "        return future_forecasts\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"   ❌ Future forecast error: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "def create_future_exogenous_data(train_data, exog_features, horizon):\n",
    "    \"\"\"\n",
    "    Create future exogenous variable values for forecasting\n",
    "    \"\"\"\n",
    "    future_data = []\n",
    "    \n",
    "    for unique_id in train_data['unique_id'].unique():\n",
    "        # Get last date for this product\n",
    "        product_data = train_data[train_data['unique_id'] == unique_id]\n",
    "        last_date = product_data['ds'].max()\n",
    "        \n",
    "        # Generate future dates\n",
    "        future_dates = pd.date_range(\n",
    "            start=last_date + pd.Timedelta(days=1),\n",
    "            periods=horizon,\n",
    "            freq='D'\n",
    "        )\n",
    "        \n",
    "        for date in future_dates:\n",
    "            row = {'unique_id': unique_id, 'ds': date}\n",
    "            \n",
    "            # Add predictable calendar features\n",
    "            if 'year' in exog_features:\n",
    "                row['year'] = date.year\n",
    "            if 'month' in exog_features:\n",
    "                row['month'] = date.month\n",
    "            if 'day_of_week' in exog_features:\n",
    "                row['day_of_week'] = date.dayofweek\n",
    "            if 'is_weekend' in exog_features:\n",
    "                row['is_weekend'] = int(date.dayofweek >= 5)\n",
    "            if 'is_month_start' in exog_features:\n",
    "                row['is_month_start'] = int(date.is_month_start)\n",
    "            if 'is_month_end' in exog_features:\n",
    "                row['is_month_end'] = int(date.is_month_end)\n",
    "            if 'is_quarter_start' in exog_features:\n",
    "                row['is_quarter_start'] = int(date.is_quarter_start)\n",
    "            if 'is_quarter_end' in exog_features:\n",
    "                row['is_quarter_end'] = int(date.is_quarter_end)\n",
    "            \n",
    "            # Add holiday indicators (extend for future years)\n",
    "            if 'is_republic_day' in exog_features:\n",
    "                row['is_republic_day'] = int(date.strftime('%m-%d') == '01-26')\n",
    "            if 'is_holi' in exog_features:\n",
    "                row['is_holi'] = 0  # Add logic for variable date holidays\n",
    "            if 'is_diwali' in exog_features:\n",
    "                row['is_diwali'] = 0  # Add logic for variable date holidays\n",
    "            \n",
    "            future_data.append(row)\n",
    "    \n",
    "    return pd.DataFrame(future_data)\n",
    "\n",
    "# def create_future_exogenous_data(train_data, exog_features, horizon):\n",
    "#     \"\"\"\n",
    "#     Create future exogenous variable values for forecasting\n",
    "#     \"\"\"\n",
    "#     future_data = []\n",
    "    \n",
    "#     for unique_id in train_data['unique_id'].unique():\n",
    "#         # Get last date for this product\n",
    "#         product_data = train_data[train_data['unique_id'] == unique_id]\n",
    "#         last_date = product_data['ds'].max()\n",
    "        \n",
    "#         # Generate future dates\n",
    "#         future_dates = pd.date_range(\n",
    "#             start=last_date + pd.Timedelta(days=1),\n",
    "#             periods=horizon,\n",
    "#             freq='D'\n",
    "#         )\n",
    "        \n",
    "#         for date in future_dates:\n",
    "#             row = {'unique_id': unique_id, 'ds': date}\n",
    "            \n",
    "#             # Add predictable calendar features\n",
    "#             row['year'] = date.year\n",
    "#             row['month'] = date.month\n",
    "#             row['day_of_week'] = date.dayofweek\n",
    "#             row['is_weekend'] = int(date.dayofweek >= 5)\n",
    "#             row['is_month_start'] = int(date.is_month_start)\n",
    "#             row['is_month_end'] = int(date.is_month_end)\n",
    "#             row['is_quarter_start'] = int(date.is_quarter_start)\n",
    "#             row['is_quarter_end'] = int(date.is_quarter_end)\n",
    "            \n",
    "#             # Add holiday indicators (extend for future years)\n",
    "#             row['is_republic_day'] = int(date.strftime('%m-%d') == '01-26')\n",
    "#             row['is_holi'] = 0  # Add logic for variable date holidays\n",
    "#             row['is_diwali'] = 0  # Add logic for variable date holidays\n",
    "            \n",
    "#             future_data.append(row)\n",
    "    \n",
    "#     return pd.DataFrame(future_data)\n",
    "\n",
    "\n",
    "def evaluate_forecasts(forecasts, test_data):\n",
    "    \"\"\"\n",
    "    Evaluate forecast performance with improved MAPE handling\n",
    "    \"\"\"\n",
    "    print(\"   • Calculating performance metrics...\")\n",
    "    \n",
    "    # Fix datetime types\n",
    "    forecasts['ds'] = pd.to_datetime(forecasts['ds'])\n",
    "    test_data['ds'] = pd.to_datetime(test_data['ds'])\n",
    "    \n",
    "    # Merge forecasts with test data\n",
    "    evaluation_data = pd.merge(\n",
    "        test_data,\n",
    "        forecasts[['ds', 'unique_id', 'TimeGPT']],\n",
    "        on=['ds', 'unique_id'],\n",
    "        how='inner'\n",
    "    )\n",
    "    \n",
    "    if len(evaluation_data) == 0:\n",
    "        print(\"   ❌ No matching data for evaluation\")\n",
    "        return None\n",
    "    \n",
    "    # Check for zero values in actual data\n",
    "    zero_count = (evaluation_data['y'] == 0).sum()\n",
    "    near_zero_count = (np.abs(evaluation_data['y']) < 1e-8).sum()\n",
    "    \n",
    "    print(f\"   📊 Data quality check:\")\n",
    "    print(f\"      • Zero values in actual data: {zero_count}\")\n",
    "    print(f\"      • Near-zero values: {near_zero_count}\")\n",
    "    \n",
    "    # Calculate metrics by product\n",
    "    product_metrics = []\n",
    "    \n",
    "    for product_id in evaluation_data['unique_id'].unique():\n",
    "        product_eval = evaluation_data[evaluation_data['unique_id'] == product_id]\n",
    "        \n",
    "        if len(product_eval) > 0:\n",
    "            # Standard metrics\n",
    "            mse = mean_squared_error(product_eval['y'], product_eval['TimeGPT'])\n",
    "            mae = mean_absolute_error(product_eval['y'], product_eval['TimeGPT'])\n",
    "            rmse = np.sqrt(mse)\n",
    "            \n",
    "            # Improved MAPE calculation with zero handling\n",
    "            actual_values = product_eval['y'].values\n",
    "            predicted_values = product_eval['TimeGPT'].values\n",
    "            \n",
    "            # Method 1: Exclude zero values from MAPE calculation\n",
    "            non_zero_mask = actual_values != 0\n",
    "            if non_zero_mask.sum() > 0:\n",
    "                mape = np.mean(np.abs((actual_values[non_zero_mask] - predicted_values[non_zero_mask]) / actual_values[non_zero_mask])) * 100\n",
    "                effective_points = non_zero_mask.sum()\n",
    "            else:\n",
    "                mape = np.inf  # Still infinity if all values are zero\n",
    "                effective_points = 0\n",
    "            \n",
    "            # Alternative: sMAPE (Symmetric MAPE) - more robust to zeros\n",
    "            smape = np.mean(2 * np.abs(actual_values - predicted_values) / \n",
    "                           (np.abs(actual_values) + np.abs(predicted_values))) * 100\n",
    "            \n",
    "            # Alternative: WAPE (Weighted Absolute Percentage Error)\n",
    "            if actual_values.sum() != 0:\n",
    "                wape = np.sum(np.abs(actual_values - predicted_values)) / np.sum(np.abs(actual_values)) * 100\n",
    "            else:\n",
    "                wape = np.inf\n",
    "            \n",
    "            product_metrics.append({\n",
    "                'product': product_id,\n",
    "                'mse': mse,\n",
    "                'mae': mae,\n",
    "                'rmse': rmse,\n",
    "                'mape': mape,\n",
    "                'smape': smape,  # Symmetric MAPE - better for zeros\n",
    "                'wape': wape,    # Weighted APE - alternative metric\n",
    "                'data_points': len(product_eval),\n",
    "                'effective_points': effective_points,  # Points used in MAPE calculation\n",
    "                'zero_values': (actual_values == 0).sum()\n",
    "            })\n",
    "    \n",
    "    metrics_df = pd.DataFrame(product_metrics)\n",
    "    \n",
    "    # Calculate overall statistics using alternative metrics when MAPE is problematic\n",
    "    finite_mape_mask = np.isfinite(metrics_df['mape'])\n",
    "    \n",
    "    if finite_mape_mask.sum() > 0:\n",
    "        avg_mape = metrics_df.loc[finite_mape_mask, 'mape'].mean()\n",
    "        mape_note = f\"(calculated from {finite_mape_mask.sum()}/{len(metrics_df)} products)\"\n",
    "    else:\n",
    "        avg_mape = np.inf\n",
    "        mape_note = \"(all products have zero values - use sMAPE instead)\"\n",
    "    \n",
    "    overall_stats = {\n",
    "        'avg_mse': metrics_df['mse'].mean(),\n",
    "        'avg_mae': metrics_df['mae'].mean(),\n",
    "        'avg_rmse': metrics_df['rmse'].mean(),\n",
    "        'avg_mape': avg_mape,\n",
    "        'avg_smape': metrics_df['smape'].mean(),  # More reliable alternative\n",
    "        'avg_wape': metrics_df[np.isfinite(metrics_df['wape'])]['wape'].mean(),\n",
    "        'total_products': len(metrics_df),\n",
    "        'total_points': len(evaluation_data),\n",
    "        'products_with_zeros': (metrics_df['zero_values'] > 0).sum()\n",
    "    }\n",
    "    \n",
    "    print(f\"   ✅ Evaluation complete: {len(metrics_df)} products evaluated\")\n",
    "    print(f\"   📊 Average MAE: {overall_stats['avg_mae']:.2f}\")\n",
    "    print(f\"   📊 Average RMSE: {overall_stats['avg_rmse']:.2f}\")\n",
    "    print(f\"   📊 Average sMAPE: {overall_stats['avg_smape']:.2f}% (recommended)\")\n",
    "    print(f\"   📊 Average MAPE: {overall_stats['avg_mape']:.2f}% {mape_note}\")\n",
    "    print(f\"   ⚠️  Products with zero values: {overall_stats['products_with_zeros']}\")\n",
    "    \n",
    "    return {\n",
    "        'product_metrics': metrics_df,\n",
    "        'overall_stats': overall_stats,\n",
    "        'evaluation_data': evaluation_data\n",
    "    }\n",
    "\n",
    "def create_comprehensive_visualizations(full_data, forecasts, test_data, evaluation_results):\n",
    "    \"\"\"\n",
    "    Create comprehensive visualizations using TimeGPT's built-in plotting\n",
    "    \"\"\"\n",
    "    print(\"   • Creating TimeGPT visualization...\")\n",
    "    \n",
    "    if evaluation_results is None:\n",
    "        print(\"   ❌ No evaluation data for visualization\")\n",
    "        return\n",
    "    \n",
    "    # Use TimeGPT's built-in plotting function\n",
    "    try:\n",
    "        # Plot using TimeGPT's native plotting with zoomed view\n",
    "        timegpt.plot(\n",
    "            test_data,\n",
    "            forecasts,\n",
    "            models=[\"TimeGPT\"],\n",
    "            level=[90],\n",
    "            time_col=\"ds\",\n",
    "            target_col=\"y\",\n",
    "            id_col=\"unique_id\",\n",
    "            max_insample_length=60  # Show last 60 days for context\n",
    "        )\n",
    "        \n",
    "        print(\"   ✅ TimeGPT native plot created\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"   ⚠️ TimeGPT plot failed: {e}\")\n",
    "        print(\"   🔄 Creating custom matplotlib visualization...\")\n",
    "        \n",
    "        # Fallback to custom matplotlib plot\n",
    "        create_custom_visualization(full_data, forecasts, test_data, evaluation_results)\n",
    "\n",
    "def create_custom_visualization(full_data, forecasts, test_data, evaluation_results):\n",
    "    \"\"\"\n",
    "    Create custom matplotlib visualization\n",
    "    \"\"\"\n",
    "    evaluation_data = evaluation_results['evaluation_data']\n",
    "    product_metrics = evaluation_results['product_metrics']\n",
    "    \n",
    "    # Get top 3 products for plotting\n",
    "    top_products = product_metrics.nsmallest(3, 'mape')['product'].tolist()\n",
    "    \n",
    "    fig, axes = plt.subplots(len(top_products), 1, figsize=(15, 5*len(top_products)))\n",
    "    if len(top_products) == 1:\n",
    "        axes = [axes]\n",
    "    \n",
    "    for idx, product_id in enumerate(top_products):\n",
    "        # Get data for this product\n",
    "        product_train = full_data[full_data['unique_id'] == product_id]\n",
    "        product_test = evaluation_data[evaluation_data['unique_id'] == product_id]\n",
    "        product_pred = forecasts[forecasts['unique_id'] == product_id]\n",
    "        \n",
    "        # Get metrics\n",
    "        metrics = product_metrics[product_metrics['product'] == product_id].iloc[0]\n",
    "        \n",
    "        # Find split point\n",
    "        if len(product_test) > 0:\n",
    "            test_start = product_test['ds'].min()\n",
    "            train_plot = product_train[product_train['ds'] >= test_start - pd.Timedelta(days=30)]\n",
    "        \n",
    "        # Plot training data (last 30 days)\n",
    "        axes[idx].plot(train_plot['ds'], train_plot['y'], 'b-', linewidth=2, label='Training Data', alpha=0.7)\n",
    "        \n",
    "        # Plot actual test data\n",
    "        axes[idx].plot(product_test['ds'], product_test['y'], 'go-', linewidth=3, markersize=8, label='Actual Test Data')\n",
    "        \n",
    "        # Plot predictions\n",
    "        axes[idx].plot(product_pred['ds'], product_pred['TimeGPT'], 'r--', linewidth=3, marker='s', markersize=8, label='TimeGPT Predictions')\n",
    "        \n",
    "        # Add confidence intervals if available\n",
    "        if 'TimeGPT-lo-90' in product_pred.columns:\n",
    "            axes[idx].fill_between(product_pred['ds'], product_pred['TimeGPT-lo-90'], product_pred['TimeGPT-hi-90'], color='red', alpha=0.2, label='90% Confidence')\n",
    "        \n",
    "        # Formatting\n",
    "        title = f'Product: {product_id[:50]}...\\n'\n",
    "        title += f'MAPE: {metrics[\"mape\"]:.1f}% | MAE: {metrics[\"mae\"]:.1f}'\n",
    "        \n",
    "        axes[idx].set_title(title, fontsize=12, fontweight='bold')\n",
    "        axes[idx].set_xlabel('Date')\n",
    "        axes[idx].set_ylabel('Sales')\n",
    "        axes[idx].legend()\n",
    "        axes[idx].grid(True, alpha=0.3)\n",
    "        \n",
    "        # Format dates\n",
    "        axes[idx].xaxis.set_major_formatter(mdates.DateFormatter('%m/%d'))\n",
    "        plt.setp(axes[idx].xaxis.get_majorticklabels(), rotation=45)\n",
    "    \n",
    "    plt.suptitle('🤖 TimeGPT Forecasting Results', fontsize=16, fontweight='bold')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"   ✅ Custom visualization created\")\n",
    "\n",
    "def generate_future_forecasts(data, horizon=14):\n",
    "    \"\"\"\n",
    "    Generate future forecasts\n",
    "    \"\"\"\n",
    "    print(f\"   • Generating {horizon}-day future forecasts...\")\n",
    "    \n",
    "    try:\n",
    "        future_forecasts = timegpt.forecast(\n",
    "            df=data,\n",
    "            h=horizon,\n",
    "            freq='D',\n",
    "            level=[80, 90],\n",
    "            time_col='ds',\n",
    "            target_col='y',\n",
    "            id_col='unique_id'\n",
    "        )\n",
    "        \n",
    "        # Create summary\n",
    "        forecast_summary = future_forecasts.groupby('unique_id').agg({\n",
    "            'TimeGPT': ['mean', 'sum']\n",
    "        }).round(2)\n",
    "        \n",
    "        forecast_summary.columns = ['avg_daily', 'total_forecast']\n",
    "        forecast_summary = forecast_summary.sort_values('total_forecast', ascending=False)\n",
    "        \n",
    "        print(f\"   ✅ Future forecasts generated\")\n",
    "        print(f\"   📊 Top 5 products by projected sales:\")\n",
    "        print(forecast_summary.head())\n",
    "        \n",
    "        return future_forecasts\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"   ❌ Future forecast error: {e}\")\n",
    "        return None\n",
    "\n",
    "# Main execution function\n",
    "\n",
    "def run_timegpt_forecasting_fixed(train_data, predictable_features, test_periods=7):\n",
    "    \"\"\"\n",
    "    FIXED TimeGPT forecasting with correct exogenous variable format\n",
    "    \"\"\"\n",
    "    print(f\"   🎯 Running FIXED TimeGPT forecast for {test_periods} periods...\")\n",
    "    \n",
    "    try:\n",
    "        # ✅ CORRECT APPROACH: Keep all variables in the same dataframe\n",
    "        available_exog = [f for f in predictable_features if f in train_data.columns]\n",
    "        \n",
    "        if available_exog:\n",
    "            print(f\"   • Using exogenous variables: {available_exog}\")\n",
    "            \n",
    "            # Use the complete dataframe with target + exogenous variables\n",
    "            forecast_data = train_data[['unique_id', 'ds', 'y'] + available_exog].copy()\n",
    "            \n",
    "            print(f\"   • Forecast data shape: {forecast_data.shape}\")\n",
    "            print(f\"   • Columns: {list(forecast_data.columns)}\")\n",
    "            \n",
    "            # Generate forecasts with all variables in one dataframe\n",
    "            forecasts = timegpt.forecast(\n",
    "                df=forecast_data,     # ✅ Single dataframe with target + exogenous\n",
    "                h=test_periods,\n",
    "                freq='D',\n",
    "                level=[80, 90],\n",
    "                time_col='ds',\n",
    "                target_col='y',\n",
    "                id_col='unique_id',\n",
    "                # Fine-tuning parameters\n",
    "                finetune_steps=30,\n",
    "                finetune_loss='mae',\n",
    "                clean_ex_first=True\n",
    "            )\n",
    "            \n",
    "        else:\n",
    "            print(\"   • No exogenous variables available, using target only\")\n",
    "            forecast_data = train_data[['unique_id', 'ds', 'y']].copy()\n",
    "            \n",
    "            forecasts = timegpt.forecast(\n",
    "                df=forecast_data,\n",
    "                h=test_periods,\n",
    "                freq='D',\n",
    "                level=[80, 90],\n",
    "                time_col='ds',\n",
    "                target_col='y',\n",
    "                id_col='unique_id',\n",
    "                finetune_steps=30,\n",
    "                finetune_loss='mae'\n",
    "            )\n",
    "        \n",
    "        print(f\"   ✅ FIXED forecasts generated: {len(forecasts):,} rows\")\n",
    "        return forecasts\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"   ❌ Fixed forecasting failed: {e}\")\n",
    "        print(\"   🔄 Trying basic approach without exogenous variables...\")\n",
    "        \n",
    "        # Fallback: Use only target variables\n",
    "        try:\n",
    "            basic_data = train_data[['unique_id', 'ds', 'y']].copy()\n",
    "            \n",
    "            forecasts = timegpt.forecast(\n",
    "                df=basic_data,\n",
    "                h=test_periods,\n",
    "                freq='D',\n",
    "                level=[80, 90],\n",
    "                time_col='ds',\n",
    "                target_col='y',\n",
    "                id_col='unique_id',\n",
    "                finetune_steps=20,\n",
    "                finetune_loss='mae'\n",
    "            )\n",
    "            \n",
    "            print(f\"   ✅ Basic forecasts generated: {len(forecasts):,} rows\")\n",
    "            return forecasts\n",
    "            \n",
    "        except Exception as e2:\n",
    "            print(f\"   ❌ All approaches failed: {e2}\")\n",
    "            \n",
    "            # Final fallback - minimal configuration\n",
    "            minimal_data = train_data[['unique_id', 'ds', 'y']].head(1000)  # Reduce data size\n",
    "            \n",
    "            forecasts = timegpt.forecast(\n",
    "                df=minimal_data,\n",
    "                h=test_periods,\n",
    "                freq='D',\n",
    "                time_col='ds',\n",
    "                target_col='y',\n",
    "                id_col='unique_id'\n",
    "            )\n",
    "            \n",
    "            print(f\"   ✅ Minimal fallback forecasts: {len(forecasts):,} rows\")\n",
    "            return forecasts\n",
    "def generate_future_forecasts_fixed(data, predictable_features, horizon=14):\n",
    "    \"\"\"\n",
    "    Generate future forecasts with proper exogenous variable handling - CORRECTED VERSION\n",
    "    \"\"\"\n",
    "    print(f\"   • Generating {horizon}-day future forecasts...\")\n",
    "    \n",
    "    try:\n",
    "        # Check if exogenous variables exist\n",
    "        available_exog = [col for col in predictable_features if col in data.columns]\n",
    "        \n",
    "        if available_exog:\n",
    "            print(f\"   • Including exogenous features: {available_exog}\")\n",
    "            \n",
    "            # ✅ CORRECT APPROACH: Separate historical data and future exogenous data\n",
    "            historical_data = data[['unique_id', 'ds', 'y'] + available_exog].copy()\n",
    "            \n",
    "            # Create future exogenous data WITHOUT target variable\n",
    "            future_X_df = create_future_exogenous_data_fixed(data, available_exog, horizon)\n",
    "            \n",
    "            print(f\"   • Historical data shape: {historical_data.shape}\")\n",
    "            print(f\"   • Future exogenous data shape: {future_X_df.shape}\")\n",
    "            \n",
    "            # Use TimeGPT with separate dataframes\n",
    "            future_forecasts = timegpt.forecast(\n",
    "                df=historical_data,    # ✅ Historical data with target\n",
    "                X_df=future_X_df,      # ✅ Future exogenous data WITHOUT target\n",
    "                h=horizon,\n",
    "                freq='D',\n",
    "                level=[80, 90],\n",
    "                time_col='ds',\n",
    "                target_col='y',\n",
    "                id_col='unique_id'\n",
    "            )\n",
    "        else:\n",
    "            # No exogenous variables - standard forecasting\n",
    "            future_forecasts = timegpt.forecast(\n",
    "                df=data[['unique_id', 'ds', 'y']],\n",
    "                h=horizon,\n",
    "                freq='D',\n",
    "                level=[80, 90],\n",
    "                time_col='ds',\n",
    "                target_col='y',\n",
    "                id_col='unique_id'\n",
    "            )\n",
    "        \n",
    "        # Create summary\n",
    "        forecast_summary = future_forecasts.groupby('unique_id').agg({\n",
    "            'TimeGPT': ['mean', 'sum']\n",
    "        }).round(2)\n",
    "        \n",
    "        forecast_summary.columns = ['avg_daily', 'total_forecast']\n",
    "        forecast_summary = forecast_summary.sort_values('total_forecast', ascending=False)\n",
    "        \n",
    "        print(f\"   ✅ Future forecasts generated\")\n",
    "        print(f\"   📊 Top 5 products by projected sales:\")\n",
    "        print(forecast_summary.head())\n",
    "        \n",
    "        return future_forecasts\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"   ❌ Future forecast error: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "def create_future_exogenous_data_fixed(train_data, exog_features, horizon):\n",
    "    \"\"\"\n",
    "    Create future exogenous variable values for forecasting - FIXED VERSION\n",
    "    \"\"\"\n",
    "    future_data = []\n",
    "    \n",
    "    for unique_id in train_data['unique_id'].unique():\n",
    "        # Get last date for this product\n",
    "        product_data = train_data[train_data['unique_id'] == unique_id]\n",
    "        last_date = product_data['ds'].max()\n",
    "        \n",
    "        # Generate future dates\n",
    "        future_dates = pd.date_range(\n",
    "            start=last_date + pd.Timedelta(days=1),\n",
    "            periods=horizon,\n",
    "            freq='D'\n",
    "        )\n",
    "        \n",
    "        for date in future_dates:\n",
    "            row = {'unique_id': unique_id, 'ds': date}  # y is None for future periods\n",
    "            \n",
    "            # Add predictable calendar features\n",
    "            if 'year' in exog_features:\n",
    "                row['year'] = date.year\n",
    "            if 'month' in exog_features:\n",
    "                row['month'] = date.month\n",
    "            if 'day_of_week' in exog_features:\n",
    "                row['day_of_week'] = date.dayofweek\n",
    "            if 'is_weekend' in exog_features:\n",
    "                row['is_weekend'] = int(date.dayofweek >= 5)\n",
    "            if 'is_month_start' in exog_features:\n",
    "                row['is_month_start'] = int(date.is_month_start)\n",
    "            if 'is_month_end' in exog_features:\n",
    "                row['is_month_end'] = int(date.is_month_end)\n",
    "            if 'is_quarter_start' in exog_features:\n",
    "                row['is_quarter_start'] = int(date.is_quarter_start)\n",
    "            if 'is_quarter_end' in exog_features:\n",
    "                row['is_quarter_end'] = int(date.is_quarter_end)\n",
    "            \n",
    "            # Add holiday indicators (extend for future years)\n",
    "            if 'is_republic_day' in exog_features:\n",
    "                row['is_republic_day'] = int(date.strftime('%m-%d') == '01-26')\n",
    "            if 'is_holi' in exog_features:\n",
    "                row['is_holi'] = 0  # Add logic for variable date holidays\n",
    "            if 'is_diwali' in exog_features:\n",
    "                row['is_diwali'] = 0  # Add logic for variable date holidays\n",
    "            \n",
    "            future_data.append(row)\n",
    "    \n",
    "    return pd.DataFrame(future_data)\n",
    "\n",
    "\n",
    "# Updated main execution function\n",
    "def complete_timegpt_pipeline_with_exogenous_fixed(df_sorted, top_n_products=8):\n",
    "    \"\"\"\n",
    "    Complete end-to-end TimeGPT forecasting pipeline with exogenous features - FIXED VERSION\n",
    "    \"\"\"\n",
    "    print(\"🚀 STARTING FIXED TIMEGPT PIPELINE WITH EXOGENOUS FEATURES\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    # Step 1: Enhanced Data Cleaning and Preparation with Exogenous Features\n",
    "    print(\"\\n📋 STEP 1: DATA CLEANING WITH EXOGENOUS FEATURES\")\n",
    "    cleaned_data, predictable_features = clean_and_prepare_data_with_exogenous(df_sorted, top_n_products)\n",
    "    \n",
    "    # Step 2: Fix Frequency Issues\n",
    "    print(\"\\n🔧 STEP 2: FIXING FREQUENCY ISSUES\")\n",
    "    frequency_fixed_data = fix_frequency_issues_complete(cleaned_data)\n",
    "    \n",
    "    # Step 3: Train-Test Split\n",
    "    print(\"\\n✂️ STEP 3: TRAIN-TEST SPLIT\")\n",
    "    train_data, test_data = create_train_test_split(frequency_fixed_data)\n",
    "    \n",
    "    # Step 4: TimeGPT Forecasting with Exogenous Features - FIXED\n",
    "    print(\"\\n🤖 ST`EP 4: TIMEGPT FORECASTING WITH EXOGENOUS FEATURES - FIXED\")\n",
    "    forecasts = run_timegpt_forecasting_fixed(train_data, predictable_features, test_periods=7)\n",
    "    \n",
    "    # Step 5: Evaluation\n",
    "    print(\"\\n📊 STEP 5: MODEL EVALUATION\")\n",
    "    evaluation_results = evaluate_forecasts(forecasts, test_data)\n",
    "    \n",
    "    # Step 6: Visualization\n",
    "    print(\"\\n📈 STEP 6: CREATING VISUALIZATIONS\")\n",
    "    create_comprehensive_visualizations(frequency_fixed_data, forecasts, test_data, evaluation_results)\n",
    "    \n",
    "    # Step 7: Future Forecasting - FIXED\n",
    "    print(\"\\n🔮 STEP 7: FUTURE FORECASTING - FIXED\")\n",
    "    future_forecasts = generate_future_forecasts_fixed(frequency_fixed_data, predictable_features, horizon=14)\n",
    "    \n",
    "    return {\n",
    "        'cleaned_data': frequency_fixed_data,\n",
    "        'train_data': train_data,\n",
    "        'test_data': test_data,\n",
    "        'forecasts': forecasts,\n",
    "        'evaluation': evaluation_results,\n",
    "        'future_forecasts': future_forecasts,\n",
    "        'exogenous_features': predictable_features\n",
    "    }\n",
    "\n",
    "\n",
    "\n",
    "results = complete_timegpt_pipeline_with_exogenous_fixed(df_sorted, top_n_products=8)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Performance: 87.1% MAPE\n"
     ]
    }
   ],
   "source": [
    "if results:\n",
    "    cleaned_data = results['cleaned_data']\n",
    "    forecasts = results['forecasts']\n",
    "    evaluation = results['evaluation']\n",
    "    future_forecasts = results['future_forecasts']\n",
    "    \n",
    "    # Print performance summary\n",
    "    if evaluation:\n",
    "        print(f\"Model Performance: {evaluation['overall_stats']['avg_mape']:.1f}% MAPE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Date range: 2022-09-30 → 2024-11-29\n",
      "Overall inferred frequency: None\n",
      "Unique products: 1883\n",
      "History length per product (days):\n",
      "  • min: 1,  median: 18.0,  max: 792\n",
      "\n",
      "TOTAL_NET_SALES distribution:\n",
      "count    823225.000000\n",
      "mean        723.379285\n",
      "std         798.541777\n",
      "min       -6999.000000\n",
      "25%         399.000000\n",
      "50%         599.000000\n",
      "75%         915.570000\n",
      "max      214500.000000\n",
      "Name: TOTAL_NET_SALES, dtype: float64\n",
      "\n",
      "Missing values per column:\n",
      "Unnamed: 0            0\n",
      "BILLING_DATE          0\n",
      "SITE                  0\n",
      "BRAND                 0\n",
      "FORMAT_DESC           0\n",
      "MH_SEGMENT            0\n",
      "MH_FAMILY             0\n",
      "MH_CLASS              0\n",
      "MH_BRICK              0\n",
      "ARTICLE               0\n",
      "TOTAL_BILLING_QTY     0\n",
      "TOTAL_GROSS_MARGIN    0\n",
      "TOTAL_MARKDOWN        0\n",
      "TOTAL_NET_SALES       0\n",
      "TOTAL_GROSS_SALES     0\n",
      "TOTAL_DISCOUNT        0\n",
      "PRODUCT_KEY           0\n",
      "dtype: int64\n",
      "\n",
      "Days with zero sales: 1233\n",
      "\n",
      "Available columns for exogenous features:\n",
      "['Unnamed: 0', 'SITE', 'BRAND', 'FORMAT_DESC', 'MH_SEGMENT', 'MH_FAMILY', 'MH_CLASS', 'MH_BRICK', 'ARTICLE', 'TOTAL_BILLING_QTY', 'TOTAL_GROSS_MARGIN', 'TOTAL_MARKDOWN', 'TOTAL_GROSS_SALES', 'TOTAL_DISCOUNT']\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# 1. Ensure BILLING_DATE is datetime\n",
    "df = df_sorted.copy()\n",
    "df['BILLING_DATE'] = pd.to_datetime(df['BILLING_DATE'])\n",
    "\n",
    "# 2. Date range\n",
    "min_date = df['BILLING_DATE'].min()\n",
    "max_date = df['BILLING_DATE'].max()\n",
    "print(f\"Date range: {min_date.date()} → {max_date.date()}\")\n",
    "\n",
    "# 3. Granularity (frequency)  \n",
    "#    We infer on the full date index; if irregular, you may want to sample a single product.\n",
    "freq = pd.infer_freq(df.set_index('BILLING_DATE').sort_index().index)\n",
    "print(f\"Overall inferred frequency: {freq}\")\n",
    "\n",
    "# 4. Unique series count & typical length\n",
    "num_products = df['PRODUCT_KEY'].nunique()\n",
    "lengths = df.groupby('PRODUCT_KEY')['BILLING_DATE'].nunique()\n",
    "print(f\"Unique products: {num_products}\")\n",
    "print(f\"History length per product (days):\")\n",
    "print(f\"  • min: {lengths.min()},  median: {lengths.median()},  max: {lengths.max()}\")\n",
    "\n",
    "# 5. Target distribution\n",
    "print(\"\\nTOTAL_NET_SALES distribution:\")\n",
    "print(df['TOTAL_NET_SALES'].describe())\n",
    "\n",
    "# 6. Missing / zero values\n",
    "print(\"\\nMissing values per column:\")\n",
    "print(df.isna().sum())\n",
    "print(f\"\\nDays with zero sales: {(df['TOTAL_NET_SALES'] == 0).sum()}\")\n",
    "\n",
    "# 7. List of potential covariates\n",
    "print(\"\\nAvailable columns for exogenous features:\")\n",
    "print([c for c in df.columns if c not in ['BILLING_DATE', 'TOTAL_NET_SALES', 'PRODUCT_KEY']])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kept 552 products with ≥60 days of data\n"
     ]
    }
   ],
   "source": [
    "\n",
    "MIN_HISTORY = 60\n",
    "hist_lengths = df.groupby('PRODUCT_KEY')['BILLING_DATE'].nunique()\n",
    "valid_keys = hist_lengths[hist_lengths >= MIN_HISTORY].index\n",
    "df = df[df['PRODUCT_KEY'].isin(valid_keys)]\n",
    "print(f\"Kept {df['PRODUCT_KEY'].nunique()} products with ≥{MIN_HISTORY} days of data\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_calendar_features(df):\n",
    "    \"\"\"Create comprehensive calendar features for Indian retail\"\"\"\n",
    "    df = df.copy()\n",
    "    df['BILLING_DATE'] = pd.to_datetime(df['BILLING_DATE'])\n",
    "    \n",
    "    # Basic temporal features\n",
    "    df['year'] = df['BILLING_DATE'].dt.year\n",
    "    df['month'] = df['BILLING_DATE'].dt.month\n",
    "    df['day_of_week'] = df['BILLING_DATE'].dt.dayofweek\n",
    "    df['day_of_year'] = df['BILLING_DATE'].dt.dayofyear\n",
    "    df['week_of_year'] = df['BILLING_DATE'].dt.isocalendar().week\n",
    "    df['quarter'] = df['BILLING_DATE'].dt.quarter\n",
    "    \n",
    "    # Weekend indicator\n",
    "    df['is_weekend'] = (df['day_of_week'] >= 5).astype(int)\n",
    "    \n",
    "    # Month-end/start effects\n",
    "    df['is_month_start'] = df['BILLING_DATE'].dt.is_month_start.astype(int)\n",
    "    df['is_month_end'] = df['BILLING_DATE'].dt.is_month_end.astype(int)\n",
    "    df['is_quarter_start'] = df['BILLING_DATE'].dt.is_quarter_start.astype(int)\n",
    "    df['is_quarter_end'] = df['BILLING_DATE'].dt.is_quarter_end.astype(int)\n",
    "    \n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'PRODUCT_KEY'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[64]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      1\u001b[39m timegpt_data = (\n\u001b[32m      2\u001b[39m     \u001b[43mdf\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrename\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m=\u001b[49m\u001b[43m{\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mBILLING_DATE\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mds\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mTOTAL_NET_SALES\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43my\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      3\u001b[39m \u001b[43m      \u001b[49m\u001b[43m.\u001b[49m\u001b[43mset_index\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mds\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m \u001b[43m      \u001b[49m\u001b[43m.\u001b[49m\u001b[43mgroupby\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mPRODUCT_KEY\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m[\u001b[33m'\u001b[39m\u001b[33my\u001b[39m\u001b[33m'\u001b[39m]\n\u001b[32m      5\u001b[39m       .resample(\u001b[33m'\u001b[39m\u001b[33mW-MON\u001b[39m\u001b[33m'\u001b[39m)   \u001b[38;5;66;03m# weekly bins starting Monday\u001b[39;00m\n\u001b[32m      6\u001b[39m       .sum()\n\u001b[32m      7\u001b[39m       .reset_index()\n\u001b[32m      8\u001b[39m       .rename(columns={\u001b[33m'\u001b[39m\u001b[33mPRODUCT_KEY\u001b[39m\u001b[33m'\u001b[39m:\u001b[33m'\u001b[39m\u001b[33munique_id\u001b[39m\u001b[33m'\u001b[39m})\n\u001b[32m      9\u001b[39m )\n\u001b[32m     10\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mWeekly series: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtimegpt_data[\u001b[33m'\u001b[39m\u001b[33munique_id\u001b[39m\u001b[33m'\u001b[39m].nunique()\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m products, \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(timegpt_data)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m rows\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/agent_env/lib/python3.11/site-packages/pandas/core/frame.py:9190\u001b[39m, in \u001b[36mDataFrame.groupby\u001b[39m\u001b[34m(self, by, axis, level, as_index, sort, group_keys, observed, dropna)\u001b[39m\n\u001b[32m   9187\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m level \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m by \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   9188\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mYou have to supply one of \u001b[39m\u001b[33m'\u001b[39m\u001b[33mby\u001b[39m\u001b[33m'\u001b[39m\u001b[33m and \u001b[39m\u001b[33m'\u001b[39m\u001b[33mlevel\u001b[39m\u001b[33m'\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m9190\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mDataFrameGroupBy\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   9191\u001b[39m \u001b[43m    \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   9192\u001b[39m \u001b[43m    \u001b[49m\u001b[43mkeys\u001b[49m\u001b[43m=\u001b[49m\u001b[43mby\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   9193\u001b[39m \u001b[43m    \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m=\u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   9194\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   9195\u001b[39m \u001b[43m    \u001b[49m\u001b[43mas_index\u001b[49m\u001b[43m=\u001b[49m\u001b[43mas_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   9196\u001b[39m \u001b[43m    \u001b[49m\u001b[43msort\u001b[49m\u001b[43m=\u001b[49m\u001b[43msort\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   9197\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgroup_keys\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup_keys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   9198\u001b[39m \u001b[43m    \u001b[49m\u001b[43mobserved\u001b[49m\u001b[43m=\u001b[49m\u001b[43mobserved\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   9199\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdropna\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdropna\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   9200\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/agent_env/lib/python3.11/site-packages/pandas/core/groupby/groupby.py:1329\u001b[39m, in \u001b[36mGroupBy.__init__\u001b[39m\u001b[34m(self, obj, keys, axis, level, grouper, exclusions, selection, as_index, sort, group_keys, observed, dropna)\u001b[39m\n\u001b[32m   1326\u001b[39m \u001b[38;5;28mself\u001b[39m.dropna = dropna\n\u001b[32m   1328\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m grouper \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1329\u001b[39m     grouper, exclusions, obj = \u001b[43mget_grouper\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1330\u001b[39m \u001b[43m        \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1331\u001b[39m \u001b[43m        \u001b[49m\u001b[43mkeys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1332\u001b[39m \u001b[43m        \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m=\u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1333\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1334\u001b[39m \u001b[43m        \u001b[49m\u001b[43msort\u001b[49m\u001b[43m=\u001b[49m\u001b[43msort\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1335\u001b[39m \u001b[43m        \u001b[49m\u001b[43mobserved\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobserved\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mlib\u001b[49m\u001b[43m.\u001b[49m\u001b[43mno_default\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobserved\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1336\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdropna\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdropna\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1337\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1339\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m observed \u001b[38;5;129;01mis\u001b[39;00m lib.no_default:\n\u001b[32m   1340\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28many\u001b[39m(ping._passed_categorical \u001b[38;5;28;01mfor\u001b[39;00m ping \u001b[38;5;129;01min\u001b[39;00m grouper.groupings):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/agent_env/lib/python3.11/site-packages/pandas/core/groupby/grouper.py:1043\u001b[39m, in \u001b[36mget_grouper\u001b[39m\u001b[34m(obj, key, axis, level, sort, observed, validate, dropna)\u001b[39m\n\u001b[32m   1041\u001b[39m         in_axis, level, gpr = \u001b[38;5;28;01mFalse\u001b[39;00m, gpr, \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1042\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1043\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(gpr)\n\u001b[32m   1044\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(gpr, Grouper) \u001b[38;5;129;01mand\u001b[39;00m gpr.key \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   1045\u001b[39m     \u001b[38;5;66;03m# Add key to exclusions\u001b[39;00m\n\u001b[32m   1046\u001b[39m     exclusions.add(gpr.key)\n",
      "\u001b[31mKeyError\u001b[39m: 'PRODUCT_KEY'"
     ]
    }
   ],
   "source": [
    "timegpt_data = (\n",
    "    df.rename(columns={'BILLING_DATE':'ds', 'TOTAL_NET_SALES':'y'})\n",
    "      .set_index('ds')\n",
    "      .groupby('PRODUCT_KEY')['y']\n",
    "      .resample('W-MON')   # weekly bins starting Monday\n",
    "      .sum()\n",
    "      .reset_index()\n",
    "      .rename(columns={'PRODUCT_KEY':'unique_id'})\n",
    ")\n",
    "print(f\"Weekly series: {timegpt_data['unique_id'].nunique()} products, {len(timegpt_data)} rows\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kept 552 products with ≥60 days of data\n",
      "Weekly series: 552 products, 42417 rows\n",
      "Exogenous features X_df: 42417 rows, columns: ['TOTAL_BILLING_QTY', 'TOTAL_DISCOUNT', 'dayofweek', 'is_holiday']\n",
      "\n",
      "Sample of y-series:\n",
      "                                           unique_id         ds       y\n",
      "0  02156 | PERSONAL CARE | P'SONAL HYGN | GENRL P... 2023-07-17   750.0\n",
      "1  02156 | PERSONAL CARE | P'SONAL HYGN | GENRL P... 2023-07-24  1125.0\n",
      "2  02156 | PERSONAL CARE | P'SONAL HYGN | GENRL P... 2023-07-31  1125.0\n",
      "3  02156 | PERSONAL CARE | P'SONAL HYGN | GENRL P... 2023-08-07   375.0\n",
      "4  02156 | PERSONAL CARE | P'SONAL HYGN | GENRL P... 2023-08-14     0.0\n",
      "\n",
      "Sample of X_df:\n",
      "                                           unique_id         ds  \\\n",
      "                                                                  \n",
      "0  02156 | PERSONAL CARE | P'SONAL HYGN | GENRL P... 2023-07-17   \n",
      "1  02156 | PERSONAL CARE | P'SONAL HYGN | GENRL P... 2023-07-24   \n",
      "2  02156 | PERSONAL CARE | P'SONAL HYGN | GENRL P... 2023-07-31   \n",
      "3  02156 | PERSONAL CARE | P'SONAL HYGN | GENRL P... 2023-08-07   \n",
      "4  02156 | PERSONAL CARE | P'SONAL HYGN | GENRL P... 2023-08-14   \n",
      "\n",
      "  TOTAL_BILLING_QTY                                        TOTAL_DISCOUNT  \\\n",
      "  TOTAL_BILLING_QTY TOTAL_DISCOUNT dayofweek is_holiday TOTAL_BILLING_QTY   \n",
      "0                 2            0.0         5          0                 2   \n",
      "1                 3            0.0        13          0                 3   \n",
      "2                 3            0.0         6          0                 3   \n",
      "3                 1            0.0         2          0                 1   \n",
      "4                 0            0.0         0          0                 0   \n",
      "\n",
      "                                              dayofweek                 \\\n",
      "  TOTAL_DISCOUNT dayofweek is_holiday TOTAL_BILLING_QTY TOTAL_DISCOUNT   \n",
      "0            0.0         5          0               1.0            0.0   \n",
      "1            0.0        13          0               1.0            0.0   \n",
      "2            0.0         6          0               1.0            0.0   \n",
      "3            0.0         2          0               1.0            0.0   \n",
      "4            0.0         0          0               NaN            NaN   \n",
      "\n",
      "                              is_holiday                                      \n",
      "  dayofweek is_holiday TOTAL_BILLING_QTY TOTAL_DISCOUNT dayofweek is_holiday  \n",
      "0  2.500000        0.0                 2            0.0         5          0  \n",
      "1  4.333333        0.0                 3            0.0        13          0  \n",
      "2  2.000000        0.0                 3            0.0         6          0  \n",
      "3  2.000000        0.0                 1            0.0         2          0  \n",
      "4       NaN        NaN                 0            0.0         0          0  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# 1. Ensure BILLING_DATE is datetime\n",
    "df = df_sorted.copy()\n",
    "df['BILLING_DATE'] = pd.to_datetime(df['BILLING_DATE'])\n",
    "\n",
    "# 2. Filter out short series (< MIN_HISTORY days)\n",
    "MIN_HISTORY = 60\n",
    "hist_lengths = df.groupby('PRODUCT_KEY')['BILLING_DATE'].nunique()\n",
    "valid_keys = hist_lengths[hist_lengths >= MIN_HISTORY].index\n",
    "df = df[df['PRODUCT_KEY'].isin(valid_keys)]\n",
    "print(f\"Kept {df['PRODUCT_KEY'].nunique()} products with ≥{MIN_HISTORY} days of data\")\n",
    "\n",
    "# 3. Create the TimeGPT “y” series and aggregate weekly\n",
    "#    (sum of TOTAL_NET_SALES per week starting Monday)\n",
    "timegpt_data = (\n",
    "    df.rename(columns={'BILLING_DATE':'ds', 'TOTAL_NET_SALES':'y'})\n",
    "      .set_index('ds')\n",
    "      .groupby('PRODUCT_KEY')['y']\n",
    "      .resample('W-MON')   # weekly bins starting Monday\n",
    "      .sum()\n",
    "      .reset_index()\n",
    "      .rename(columns={'PRODUCT_KEY':'unique_id'})\n",
    ")\n",
    "print(f\"Weekly series: {timegpt_data['unique_id'].nunique()} products, {len(timegpt_data)} rows\")\n",
    "\n",
    "# 4. Build exogenous DataFrame X_df, aggregated the same way\n",
    "#    Here we include qty, discount, and holiday flag (example)\n",
    "#    First add weekday/month/holiday columns on the original df\n",
    "df['dayofweek'] = df['BILLING_DATE'].dt.dayofweek\n",
    "df['is_holiday'] = df['BILLING_DATE'].isin(pd.to_datetime([\n",
    "    '2023-01-26','2023-08-15','2024-01-26',  # add your full holiday list\n",
    "])).astype(int)\n",
    "\n",
    "#   Now aggregate covariates weekly\n",
    "covariates = ['TOTAL_BILLING_QTY','TOTAL_DISCOUNT','dayofweek','is_holiday']\n",
    "X_df = (\n",
    "    df.set_index('BILLING_DATE')\n",
    "      .groupby('PRODUCT_KEY')[covariates]\n",
    "      .resample('W-MON')\n",
    "      .agg({\n",
    "         'TOTAL_BILLING_QTY':'sum',\n",
    "         'TOTAL_DISCOUNT':'sum',\n",
    "         'dayofweek':'mean',      # average weekday in the week\n",
    "         'is_holiday':'sum'       # count of holidays that week\n",
    "      })\n",
    "      .reset_index()\n",
    "      .rename(columns={'PRODUCT_KEY':'unique_id', 'BILLING_DATE':'ds'})\n",
    ")\n",
    "print(f\"Exogenous features X_df: {X_df.shape[0]} rows, columns: {covariates}\")\n",
    "\n",
    "# 5. Inspect final shapes before forecasting\n",
    "print(\"\\nSample of y-series:\")\n",
    "print(timegpt_data.head())\n",
    "print(\"\\nSample of X_df:\")\n",
    "print(X_df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "DATA EXPLORATION FOR TIMEGPT PREPARATION\n",
      "============================================================\n",
      "\n",
      "1. BASIC DATA STRUCTURE\n",
      "----------------------------------------\n",
      "Dataset shape: (823225, 20)\n",
      "Columns: ['Unnamed: 0.4', 'Unnamed: 0.3', 'Unnamed: 0.2', 'Unnamed: 0.1', 'Unnamed: 0', 'BILLING_DATE', 'SITE', 'BRAND', 'FORMAT_DESC', 'MH_SEGMENT', 'MH_FAMILY', 'MH_CLASS', 'MH_BRICK', 'ARTICLE', 'TOTAL_BILLING_QTY', 'TOTAL_GROSS_MARGIN', 'TOTAL_MARKDOWN', 'TOTAL_NET_SALES', 'TOTAL_GROSS_SALES', 'TOTAL_DISCOUNT']\n",
      "Data types:\n",
      "Unnamed: 0.4            int64\n",
      "Unnamed: 0.3            int64\n",
      "Unnamed: 0.2            int64\n",
      "Unnamed: 0.1            int64\n",
      "Unnamed: 0              int64\n",
      "BILLING_DATE           object\n",
      "SITE                    int64\n",
      "BRAND                  object\n",
      "FORMAT_DESC            object\n",
      "MH_SEGMENT             object\n",
      "MH_FAMILY              object\n",
      "MH_CLASS               object\n",
      "MH_BRICK               object\n",
      "ARTICLE                 int64\n",
      "TOTAL_BILLING_QTY       int64\n",
      "TOTAL_GROSS_MARGIN    float64\n",
      "TOTAL_MARKDOWN        float64\n",
      "TOTAL_NET_SALES       float64\n",
      "TOTAL_GROSS_SALES     float64\n",
      "TOTAL_DISCOUNT        float64\n",
      "dtype: object\n",
      "\n",
      "2. TIME SERIES ANALYSIS\n",
      "----------------------------------------\n",
      "Date range: 2022-09-30 00:00:00 to 2024-11-29 00:00:00\n",
      "Total days span: 791 days\n",
      "Unique dates: 792\n",
      "Missing dates: 0\n",
      "Average transactions per day: 1039.4\n",
      "Min transactions per day: 77\n",
      "Max transactions per day: 4785\n",
      "\n",
      "3. BUSINESS DIMENSIONS\n",
      "----------------------------------------\n",
      "Unique SITES: 1\n",
      "Unique BRANDS: 194\n",
      "Unique ARTICLES: 190069\n",
      "Sites: [np.int64(8013)]\n",
      "Brands: ['02156', '02338', '02370', '02378', '02556', '02558', '02883', '03023', '03054', '03500']\n",
      "\n",
      "4. AGGREGATION POSSIBILITIES\n",
      "----------------------------------------\n",
      "A. DAILY TOTALS - OVERALL:\n",
      "              TOTAL_NET_SALES  TOTAL_GROSS_SALES  TOTAL_BILLING_QTY\n",
      "BILLING_DATE                                                       \n",
      "2022-09-30          631432.35          742490.32               1020\n",
      "2022-10-01         1147608.90         1352230.50               1822\n",
      "2022-10-02         1496194.83         1762740.09               2400\n",
      "2022-10-03         1324753.03         1413277.80               1928\n",
      "2022-10-04         1436474.04         1521505.33               1998\n",
      "Daily NET SALES range: 44779.58 to 4478545.21\n",
      "\n",
      "B. DAILY TOTALS - BY SITE:\n",
      "Site 8013: 792 days, Avg daily sales: 751898.88\n",
      "\n",
      "C. DAILY TOTALS - BY BRAND:\n",
      "Brand F085: 792 days, Avg daily sales: 112058.56\n",
      "Brand 04042: 792 days, Avg daily sales: 77451.84\n",
      "Brand 04063: 792 days, Avg daily sales: 59385.74\n",
      "Brand 04017: 792 days, Avg daily sales: 56488.71\n",
      "Brand D699: 792 days, Avg daily sales: 30746.68\n",
      "\n",
      "5. DATA QUALITY CHECK\n",
      "----------------------------------------\n",
      "Missing values:\n",
      "Unnamed: 0.4          0\n",
      "Unnamed: 0.3          0\n",
      "Unnamed: 0.2          0\n",
      "Unnamed: 0.1          0\n",
      "Unnamed: 0            0\n",
      "BILLING_DATE          0\n",
      "SITE                  0\n",
      "BRAND                 0\n",
      "FORMAT_DESC           0\n",
      "MH_SEGMENT            0\n",
      "MH_FAMILY             0\n",
      "MH_CLASS              0\n",
      "MH_BRICK              0\n",
      "ARTICLE               0\n",
      "TOTAL_BILLING_QTY     0\n",
      "TOTAL_GROSS_MARGIN    0\n",
      "TOTAL_MARKDOWN        0\n",
      "TOTAL_NET_SALES       0\n",
      "TOTAL_GROSS_SALES     0\n",
      "TOTAL_DISCOUNT        0\n",
      "dtype: int64\n",
      "\n",
      "Brand data quality:\n",
      "Brands with 'nan' string: 289\n",
      "Most common brands:\n",
      "BRAND\n",
      "F085     126618\n",
      "04063     82492\n",
      "04042     69993\n",
      "04017     65978\n",
      "F133      60061\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Zero/Negative values in key metrics:\n",
      "TOTAL_NET_SALES: 1233 zeros, 12853 negative values\n",
      "TOTAL_GROSS_SALES: 1107 zeros, 12096 negative values\n",
      "TOTAL_BILLING_QTY: 1138 zeros, 12081 negative values\n",
      "\n",
      "6. POTENTIAL FORECASTING TARGETS\n",
      "----------------------------------------\n",
      "\n",
      "DAILY_TOTAL_SALES:\n",
      "  Date range: 2022-09-30 00:00:00 to 2024-11-29 00:00:00\n",
      "  Mean: 751898.88\n",
      "  Std: 467027.49\n",
      "  Sample values: {Timestamp('2022-09-30 00:00:00'): 631432.35, Timestamp('2022-10-01 00:00:00'): 1147608.9, Timestamp('2022-10-02 00:00:00'): 1496194.83}\n",
      "\n",
      "DAILY_TOTAL_QUANTITY:\n",
      "  Date range: 2022-09-30 00:00:00 to 2024-11-29 00:00:00\n",
      "  Mean: 1166.11\n",
      "  Std: 735.44\n",
      "  Sample values: {Timestamp('2022-09-30 00:00:00'): 1020, Timestamp('2022-10-01 00:00:00'): 1822, Timestamp('2022-10-02 00:00:00'): 2400}\n",
      "\n",
      "DAILY_TRANSACTION_COUNT:\n",
      "  Date range: 2022-09-30 00:00:00 to 2024-11-29 00:00:00\n",
      "  Mean: 1039.43\n",
      "  Std: 576.25\n",
      "  Sample values: {Timestamp('2022-09-30 00:00:00'): 970, Timestamp('2022-10-01 00:00:00'): 1607, Timestamp('2022-10-02 00:00:00'): 2109}\n",
      "\n",
      "7. WEEKLY PATTERNS\n",
      "----------------------------------------\n",
      "Sales by day of week:\n",
      "day_of_week\n",
      "Monday       6.859169e+07\n",
      "Tuesday      6.290919e+07\n",
      "Wednesday    6.797650e+07\n",
      "Thursday     7.114711e+07\n",
      "Friday       7.606187e+07\n",
      "Saturday     1.112550e+08\n",
      "Sunday       1.375626e+08\n",
      "Name: TOTAL_NET_SALES, dtype: float64\n",
      "\n",
      "8. TIMEGPT DATA FORMAT EXAMPLES\n",
      "----------------------------------------\n",
      "TimeGPT requires format: ['unique_id', 'ds', 'y']\n",
      "\n",
      "Option 1 - Overall daily sales:\n",
      "     unique_id         ds           y\n",
      "0  total_sales 2022-09-30   631432.35\n",
      "1  total_sales 2022-10-01  1147608.90\n",
      "2  total_sales 2022-10-02  1496194.83\n",
      "3  total_sales 2022-10-03  1324753.03\n",
      "4  total_sales 2022-10-04  1436474.04\n",
      "\n",
      "Option 2 - Daily sales by site:\n",
      "   unique_id         ds           y\n",
      "0       8013 2022-09-30   631432.35\n",
      "1       8013 2022-10-01  1147608.90\n",
      "2       8013 2022-10-02  1496194.83\n",
      "3       8013 2022-10-03  1324753.03\n",
      "4       8013 2022-10-04  1436474.04\n",
      "\n",
      "============================================================\n",
      "NEXT STEPS: Answer these questions based on the output above:\n",
      "============================================================\n",
      "1. Which forecasting target interests you most?\n",
      "   - daily_total_sales, daily_total_quantity, or daily_transaction_count?\n",
      "2. Do you want one overall forecast or separate forecasts by site/brand?\n",
      "3. How many days ahead do you want to forecast?\n",
      "4. Any data quality issues you want to address first?\n",
      "5. Do you see any concerning patterns in the date coverage?\n"
     ]
    }
   ],
   "source": [
    "#EDA\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# Load your data\n",
    "df = pd.read_csv(\"data-8013-trends.csv\")\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"DATA EXPLORATION FOR TIMEGPT PREPARATION\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Basic data info\n",
    "print(\"\\n1. BASIC DATA STRUCTURE\")\n",
    "print(\"-\" * 40)\n",
    "print(f\"Dataset shape: {df.shape}\")\n",
    "print(f\"Columns: {list(df.columns)}\")\n",
    "print(f\"Data types:\\n{df.dtypes}\")\n",
    "\n",
    "# Convert BILLING_DATE to datetime\n",
    "df['BILLING_DATE'] = pd.to_datetime(df['BILLING_DATE'])\n",
    "\n",
    "# Fix BRAND column - convert all to string to handle mixed types\n",
    "df['BRAND'] = df['BRAND'].astype(str)\n",
    "\n",
    "# Date range analysis\n",
    "print(\"\\n2. TIME SERIES ANALYSIS\")\n",
    "print(\"-\" * 40)\n",
    "print(f\"Date range: {df['BILLING_DATE'].min()} to {df['BILLING_DATE'].max()}\")\n",
    "print(f\"Total days span: {(df['BILLING_DATE'].max() - df['BILLING_DATE'].min()).days} days\")\n",
    "print(f\"Unique dates: {df['BILLING_DATE'].nunique()}\")\n",
    "\n",
    "# Check for date gaps\n",
    "date_range = pd.date_range(start=df['BILLING_DATE'].min(), \n",
    "                          end=df['BILLING_DATE'].max(), \n",
    "                          freq='D')\n",
    "missing_dates = set(date_range) - set(df['BILLING_DATE'].unique())\n",
    "print(f\"Missing dates: {len(missing_dates)}\")\n",
    "if len(missing_dates) > 0 and len(missing_dates) < 10:\n",
    "    print(f\"Missing dates: {sorted(missing_dates)[:10]}\")\n",
    "\n",
    "# Daily transaction counts\n",
    "daily_transactions = df.groupby('BILLING_DATE').size()\n",
    "print(f\"Average transactions per day: {daily_transactions.mean():.1f}\")\n",
    "print(f\"Min transactions per day: {daily_transactions.min()}\")\n",
    "print(f\"Max transactions per day: {daily_transactions.max()}\")\n",
    "\n",
    "print(\"\\n3. BUSINESS DIMENSIONS\")\n",
    "print(\"-\" * 40)\n",
    "print(f\"Unique SITES: {df['SITE'].nunique()}\")\n",
    "print(f\"Unique BRANDS: {df['BRAND'].nunique()}\")\n",
    "print(f\"Unique ARTICLES: {df['ARTICLE'].nunique()}\")\n",
    "print(f\"Sites: {sorted(df['SITE'].unique())}\")\n",
    "print(f\"Brands: {sorted(df['BRAND'].unique())[:10]}\")  # Show first 10 brands only\n",
    "\n",
    "print(\"\\n4. AGGREGATION POSSIBILITIES\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "# Daily aggregations by different levels\n",
    "print(\"A. DAILY TOTALS - OVERALL:\")\n",
    "daily_overall = df.groupby('BILLING_DATE').agg({\n",
    "    'TOTAL_NET_SALES': 'sum',\n",
    "    'TOTAL_GROSS_SALES': 'sum', \n",
    "    'TOTAL_BILLING_QTY': 'sum'\n",
    "}).round(2)\n",
    "\n",
    "print(daily_overall.head())\n",
    "print(f\"Daily NET SALES range: {daily_overall['TOTAL_NET_SALES'].min():.2f} to {daily_overall['TOTAL_NET_SALES'].max():.2f}\")\n",
    "\n",
    "print(\"\\nB. DAILY TOTALS - BY SITE:\")\n",
    "daily_by_site = df.groupby(['BILLING_DATE', 'SITE']).agg({\n",
    "    'TOTAL_NET_SALES': 'sum',\n",
    "    'TOTAL_BILLING_QTY': 'sum'\n",
    "}).reset_index()\n",
    "\n",
    "for site in df['SITE'].unique()[:3]:  # Show first 3 sites\n",
    "    site_data = daily_by_site[daily_by_site['SITE'] == site]\n",
    "    print(f\"Site {site}: {len(site_data)} days, Avg daily sales: {site_data['TOTAL_NET_SALES'].mean():.2f}\")\n",
    "\n",
    "print(\"\\nC. DAILY TOTALS - BY BRAND:\")\n",
    "daily_by_brand = df.groupby(['BILLING_DATE', 'BRAND']).agg({\n",
    "    'TOTAL_NET_SALES': 'sum',\n",
    "    'TOTAL_BILLING_QTY': 'sum'\n",
    "}).reset_index()\n",
    "\n",
    "# Get top 5 brands by total sales to avoid issues with 'nan' brands\n",
    "top_brands = df.groupby('BRAND')['TOTAL_NET_SALES'].sum().sort_values(ascending=False).head(5).index\n",
    "\n",
    "for brand in top_brands:\n",
    "    if brand != 'nan':  # Skip if brand is 'nan' string\n",
    "        brand_data = daily_by_brand[daily_by_brand['BRAND'] == brand]\n",
    "        if len(brand_data) > 0:\n",
    "            print(f\"Brand {brand}: {len(brand_data)} days, Avg daily sales: {brand_data['TOTAL_NET_SALES'].mean():.2f}\")\n",
    "\n",
    "print(\"\\n5. DATA QUALITY CHECK\")\n",
    "print(\"-\" * 40)\n",
    "print(\"Missing values:\")\n",
    "print(df.isnull().sum())\n",
    "\n",
    "print(\"\\nBrand data quality:\")\n",
    "print(f\"Brands with 'nan' string: {(df['BRAND'] == 'nan').sum()}\")\n",
    "print(f\"Most common brands:\")\n",
    "print(df['BRAND'].value_counts().head())\n",
    "\n",
    "print(\"\\nZero/Negative values in key metrics:\")\n",
    "for col in ['TOTAL_NET_SALES', 'TOTAL_GROSS_SALES', 'TOTAL_BILLING_QTY']:\n",
    "    zero_count = (df[col] == 0).sum()\n",
    "    neg_count = (df[col] < 0).sum()\n",
    "    print(f\"{col}: {zero_count} zeros, {neg_count} negative values\")\n",
    "\n",
    "print(\"\\n6. POTENTIAL FORECASTING TARGETS\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "# Create different aggregation examples\n",
    "forecasting_options = {\n",
    "    'daily_total_sales': df.groupby('BILLING_DATE')['TOTAL_NET_SALES'].sum(),\n",
    "    'daily_total_quantity': df.groupby('BILLING_DATE')['TOTAL_BILLING_QTY'].sum(),\n",
    "    'daily_transaction_count': df.groupby('BILLING_DATE').size()\n",
    "}\n",
    "\n",
    "for name, series in forecasting_options.items():\n",
    "    print(f\"\\n{name.upper()}:\")\n",
    "    print(f\"  Date range: {series.index.min()} to {series.index.max()}\")\n",
    "    print(f\"  Mean: {series.mean():.2f}\")\n",
    "    print(f\"  Std: {series.std():.2f}\")\n",
    "    print(f\"  Sample values: {series.head(3).to_dict()}\")\n",
    "\n",
    "print(\"\\n7. WEEKLY PATTERNS\")\n",
    "print(\"-\" * 40)\n",
    "df['day_of_week'] = df['BILLING_DATE'].dt.day_name()\n",
    "weekly_sales = df.groupby('day_of_week')['TOTAL_NET_SALES'].sum().reindex([\n",
    "    'Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday'\n",
    "])\n",
    "print(\"Sales by day of week:\")\n",
    "print(weekly_sales)\n",
    "\n",
    "print(\"\\n8. TIMEGPT DATA FORMAT EXAMPLES\")\n",
    "print(\"-\" * 40)\n",
    "print(\"TimeGPT requires format: ['unique_id', 'ds', 'y']\")\n",
    "print(\"\\nOption 1 - Overall daily sales:\")\n",
    "timegpt_format1 = df.groupby('BILLING_DATE')['TOTAL_NET_SALES'].sum().reset_index()\n",
    "timegpt_format1.columns = ['ds', 'y']\n",
    "timegpt_format1['unique_id'] = 'total_sales'\n",
    "timegpt_format1 = timegpt_format1[['unique_id', 'ds', 'y']]\n",
    "print(timegpt_format1.head())\n",
    "\n",
    "print(\"\\nOption 2 - Daily sales by site:\")\n",
    "timegpt_format2 = df.groupby(['SITE', 'BILLING_DATE'])['TOTAL_NET_SALES'].sum().reset_index()\n",
    "timegpt_format2.columns = ['unique_id', 'ds', 'y']\n",
    "print(timegpt_format2.head())\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"NEXT STEPS: Answer these questions based on the output above:\")\n",
    "print(\"=\"*60)\n",
    "print(\"1. Which forecasting target interests you most?\")\n",
    "print(\"   - daily_total_sales, daily_total_quantity, or daily_transaction_count?\")\n",
    "print(\"2. Do you want one overall forecast or separate forecasts by site/brand?\")\n",
    "print(\"3. How many days ahead do you want to forecast?\")\n",
    "print(\"4. Any data quality issues you want to address first?\")\n",
    "print(\"5. Do you see any concerning patterns in the date coverage?\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "agent_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
