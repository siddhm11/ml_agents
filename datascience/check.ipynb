{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nixtlats in /opt/anaconda3/envs/agent_env/lib/python3.11/site-packages (0.5.2)\n",
      "Requirement already satisfied: httpx in /opt/anaconda3/envs/agent_env/lib/python3.11/site-packages (from nixtlats) (0.28.1)\n",
      "Requirement already satisfied: pandas in /opt/anaconda3/envs/agent_env/lib/python3.11/site-packages (from nixtlats) (2.3.0)\n",
      "Requirement already satisfied: pydantic in /opt/anaconda3/envs/agent_env/lib/python3.11/site-packages (from nixtlats) (2.11.7)\n",
      "Requirement already satisfied: requests in /opt/anaconda3/envs/agent_env/lib/python3.11/site-packages (from nixtlats) (2.32.4)\n",
      "Requirement already satisfied: tenacity in /opt/anaconda3/envs/agent_env/lib/python3.11/site-packages (from nixtlats) (9.1.2)\n",
      "Requirement already satisfied: utilsforecast>=0.1.7 in /opt/anaconda3/envs/agent_env/lib/python3.11/site-packages (from nixtlats) (0.2.12)\n",
      "Requirement already satisfied: numpy in /opt/anaconda3/envs/agent_env/lib/python3.11/site-packages (from utilsforecast>=0.1.7->nixtlats) (2.3.0)\n",
      "Requirement already satisfied: packaging in /opt/anaconda3/envs/agent_env/lib/python3.11/site-packages (from utilsforecast>=0.1.7->nixtlats) (25.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/anaconda3/envs/agent_env/lib/python3.11/site-packages (from pandas->nixtlats) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/anaconda3/envs/agent_env/lib/python3.11/site-packages (from pandas->nixtlats) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/anaconda3/envs/agent_env/lib/python3.11/site-packages (from pandas->nixtlats) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in /opt/anaconda3/envs/agent_env/lib/python3.11/site-packages (from python-dateutil>=2.8.2->pandas->nixtlats) (1.17.0)\n",
      "Requirement already satisfied: anyio in /opt/anaconda3/envs/agent_env/lib/python3.11/site-packages (from httpx->nixtlats) (4.9.0)\n",
      "Requirement already satisfied: certifi in /opt/anaconda3/envs/agent_env/lib/python3.11/site-packages (from httpx->nixtlats) (2025.6.15)\n",
      "Requirement already satisfied: httpcore==1.* in /opt/anaconda3/envs/agent_env/lib/python3.11/site-packages (from httpx->nixtlats) (1.0.9)\n",
      "Requirement already satisfied: idna in /opt/anaconda3/envs/agent_env/lib/python3.11/site-packages (from httpx->nixtlats) (3.10)\n",
      "Requirement already satisfied: h11>=0.16 in /opt/anaconda3/envs/agent_env/lib/python3.11/site-packages (from httpcore==1.*->httpx->nixtlats) (0.16.0)\n",
      "Requirement already satisfied: sniffio>=1.1 in /opt/anaconda3/envs/agent_env/lib/python3.11/site-packages (from anyio->httpx->nixtlats) (1.3.1)\n",
      "Requirement already satisfied: typing_extensions>=4.5 in /opt/anaconda3/envs/agent_env/lib/python3.11/site-packages (from anyio->httpx->nixtlats) (4.14.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /opt/anaconda3/envs/agent_env/lib/python3.11/site-packages (from pydantic->nixtlats) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in /opt/anaconda3/envs/agent_env/lib/python3.11/site-packages (from pydantic->nixtlats) (2.33.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in /opt/anaconda3/envs/agent_env/lib/python3.11/site-packages (from pydantic->nixtlats) (0.4.1)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /opt/anaconda3/envs/agent_env/lib/python3.11/site-packages (from requests->nixtlats) (3.4.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/anaconda3/envs/agent_env/lib/python3.11/site-packages (from requests->nixtlats) (2.4.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install nixtlats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"data-8013-trends.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['BILLING_DATE'] = pd.to_datetime(df['BILLING_DATE'])\n",
    "\n",
    "df.to_csv( \"data-8013-trends.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from io import StringIO\n",
    "\n",
    "# Load your data (assuming it's in CSV format)\n",
    "\n",
    "# Sort by BILLING_DATE\n",
    "df_sorted = df.sort_values(by='BILLING_DATE')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸš€ STARTING COMPLETE TIMEGPT PIPELINE\n",
      "============================================================\n",
      "\n",
      "ðŸ“‹ STEP 1: DATA CLEANING AND PREPARATION\n",
      "   â€¢ Handling null values...\n",
      "   â€¢ Selecting top performing products...\n",
      "   â€¢ Aggregating daily sales...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:nixtlats.nixtla_client:Validating inputs...\n",
      "INFO:nixtlats.nixtla_client:Preprocessing dataframes...\n",
      "WARNING:nixtlats.nixtla_client:You did not provide X_df. Exogenous variables in df are ignored. To surpress this warning, please add X_df with exogenous variables: TOTAL_BILLING_QTY\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   âœ… Data prepared: 7793 rows, 10 products\n",
      "\n",
      "ðŸ”§ STEP 2: FIXING FREQUENCY ISSUES\n",
      "   â€¢ Diagnosing frequency issues...\n",
      "   âœ… Fixed frequency for 10 products\n",
      "   ðŸ“Š Final dataset: 7918 rows\n",
      "\n",
      "âœ‚ï¸ STEP 3: TRAIN-TEST SPLIT\n",
      "   â€¢ Splitting data with 14 test days...\n",
      "   âœ… Train: 7778 rows, Test: 140 rows\n",
      "\n",
      "ðŸ¤– STEP 4: TIMEGPT FORECASTING\n",
      "   â€¢ Running TimeGPT forecast for 7 periods...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:nixtlats.nixtla_client:Restricting input...\n",
      "INFO:nixtlats.nixtla_client:Calling Forecast Endpoint...\n",
      "INFO:nixtlats.nixtla_client:Validating inputs...\n",
      "INFO:nixtlats.nixtla_client:Preprocessing dataframes...\n",
      "WARNING:nixtlats.nixtla_client:You did not provide X_df. Exogenous variables in df are ignored. To surpress this warning, please add X_df with exogenous variables: TOTAL_BILLING_QTY\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   âœ… Forecasts generated: 70 rows\n",
      "\n",
      "ðŸ“Š STEP 5: MODEL EVALUATION\n",
      "   â€¢ Calculating performance metrics...\n",
      "   ðŸ“Š Data quality check:\n",
      "      â€¢ Zero values in actual data: 1\n",
      "      â€¢ Near-zero values: 1\n",
      "   âœ… Evaluation complete: 10 products evaluated\n",
      "   ðŸ“Š Average MAE: 5055.10\n",
      "   ðŸ“Š Average RMSE: 6131.99\n",
      "   ðŸ“Š Average sMAPE: 32.00% (recommended)\n",
      "   ðŸ“Š Average MAPE: 41.50% (calculated from 10/10 products)\n",
      "   âš ï¸  Products with zero values: 1\n",
      "\n",
      "ðŸ“ˆ STEP 6: CREATING VISUALIZATIONS\n",
      "   â€¢ Creating TimeGPT visualization...\n",
      "   âœ… TimeGPT native plot created\n",
      "\n",
      "ðŸ”® STEP 7: FUTURE FORECASTING\n",
      "   â€¢ Generating 14-day future forecasts...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:nixtlats.nixtla_client:The specified horizon \"h\" exceeds the model horizon. This may lead to less accurate forecasts. Please consider using a smaller horizon.\n",
      "INFO:nixtlats.nixtla_client:Restricting input...\n",
      "INFO:nixtlats.nixtla_client:Calling Forecast Endpoint...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   âœ… Future forecasts generated\n",
      "   ðŸ“Š Top 5 products by projected sales:\n",
      "                                                    avg_daily  total_forecast\n",
      "unique_id                                                                    \n",
      "F085 | WOMENS WEAR | ETHNIC WEAR | TOPWEAR | KU...   69931.85       979045.94\n",
      "04042 | MENS WEAR | SMART CASUALS | TOPS | SHIRTS    37320.84       522491.72\n",
      "04063 | MENS CASUAL | ACTIVE WEAR | BOTTOMS | T...   19227.32       269182.53\n",
      "04042 | MENS WEAR | SMART CASUALS | BOTTOMS | T...   14836.29       207708.04\n",
      "04063 | MENS CASUAL | ACTIVE WEAR | TOPS | T SH...   14433.36       202067.07\n",
      "\n",
      "ðŸŽ‰ PIPELINE COMPLETED SUCCESSFULLY!\n",
      "============================================================\n",
      "ðŸ“‹ Results Summary:\n",
      "   â€¢ Products processed: 10\n",
      "   â€¢ Training data points: 7778\n",
      "   â€¢ Test data points: 140\n",
      "   â€¢ Average MAPE: 41.50%\n"
     ]
    }
   ],
   "source": [
    "# Import required libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "from nixtlats import TimeGPT\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Initialize TimeGPT client\n",
    "timegpt = TimeGPT(token='nixak-AI31YupjpWhin07kPGKOhvW5zj8IliWOlTSWLEInpOuKHNWNhtSETsXIFgyEiYT58g3Hk0hvMFhnJpdS')  # Replace with your actual token\n",
    "\n",
    "def complete_timegpt_pipeline(df_sorted, top_n_products=10):\n",
    "    \"\"\"\n",
    "    Complete end-to-end TimeGPT forecasting pipeline\n",
    "    \"\"\"\n",
    "    print(\"ðŸš€ STARTING COMPLETE TIMEGPT PIPELINE\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Step 1: Data Cleaning and Preparation\n",
    "    print(\"\\nðŸ“‹ STEP 1: DATA CLEANING AND PREPARATION\")\n",
    "    cleaned_data = clean_and_prepare_data(df_sorted, top_n_products)\n",
    "    \n",
    "    # Step 2: Fix Frequency Issues\n",
    "    print(\"\\nðŸ”§ STEP 2: FIXING FREQUENCY ISSUES\")\n",
    "    frequency_fixed_data = fix_frequency_issues_complete(cleaned_data)\n",
    "    \n",
    "    # Step 3: Train-Test Split\n",
    "    print(\"\\nâœ‚ï¸ STEP 3: TRAIN-TEST SPLIT\")\n",
    "    train_data, test_data = create_train_test_split(frequency_fixed_data)\n",
    "    \n",
    "    # Step 4: TimeGPT Forecasting\n",
    "    print(\"\\nðŸ¤– STEP 4: TIMEGPT FORECASTING\")\n",
    "    forecasts = run_timegpt_forecasting(train_data, test_periods=7)\n",
    "    \n",
    "    # Step 5: Evaluation\n",
    "    print(\"\\nðŸ“Š STEP 5: MODEL EVALUATION\")\n",
    "    evaluation_results = evaluate_forecasts(forecasts, test_data)\n",
    "    \n",
    "    # Step 6: Visualization\n",
    "    print(\"\\nðŸ“ˆ STEP 6: CREATING VISUALIZATIONS\")\n",
    "    create_comprehensive_visualizations(frequency_fixed_data, forecasts, test_data, evaluation_results)\n",
    "    \n",
    "    # Step 7: Future Forecasting\n",
    "    print(\"\\nðŸ”® STEP 7: FUTURE FORECASTING\")\n",
    "    future_forecasts = generate_future_forecasts(frequency_fixed_data, horizon = 14)\n",
    "    \n",
    "    return {\n",
    "        'cleaned_data': frequency_fixed_data,\n",
    "        'train_data': train_data,\n",
    "        'test_data': test_data,\n",
    "        'forecasts': forecasts,\n",
    "        'evaluation': evaluation_results,\n",
    "        'future_forecasts': future_forecasts\n",
    "    }\n",
    "\n",
    "def clean_and_prepare_data(df_sorted, top_n_products=10):\n",
    "    \"\"\"\n",
    "    Clean and prepare data for TimeGPT\n",
    "    \"\"\"\n",
    "    print(\"   â€¢ Handling null values...\")\n",
    "    \n",
    "    # Handle null values in BRAND column\n",
    "    if df_sorted['BRAND'].isnull().sum() > 0:\n",
    "        if pd.api.types.is_categorical_dtype(df_sorted['BRAND']):\n",
    "            df_sorted['BRAND'] = df_sorted['BRAND'].cat.add_categories(['UNKNOWN_BRAND'])\n",
    "        df_sorted['BRAND'] = df_sorted['BRAND'].fillna('UNKNOWN_BRAND')\n",
    "    \n",
    "    # Create PRODUCT_KEY if not exists\n",
    "    if 'PRODUCT_KEY' not in df_sorted.columns:\n",
    "        df_sorted['PRODUCT_KEY'] = (\n",
    "            df_sorted['BRAND'].astype(str) + ' | ' + \n",
    "            df_sorted['MH_SEGMENT'].astype(str) + ' | ' + \n",
    "            df_sorted['MH_FAMILY'].astype(str) + ' | ' + \n",
    "            df_sorted['MH_CLASS'].astype(str) + ' | ' + \n",
    "            df_sorted['MH_BRICK'].astype(str)\n",
    "        )\n",
    "    \n",
    "    print(\"   â€¢ Selecting top performing products...\")\n",
    "    \n",
    "    # Select top products by total sales\n",
    "    top_products = (df_sorted.groupby('PRODUCT_KEY')['TOTAL_NET_SALES'].sum()\n",
    "                   .nlargest(top_n_products).index.tolist())\n",
    "    \n",
    "    # Filter data for selected products\n",
    "    filtered_df = df_sorted[df_sorted['PRODUCT_KEY'].isin(top_products)].copy()\n",
    "    \n",
    "    print(\"   â€¢ Aggregating daily sales...\")\n",
    "    \n",
    "    # Aggregate daily sales by product\n",
    "    daily_sales = filtered_df.groupby(['BILLING_DATE', 'PRODUCT_KEY']).agg({\n",
    "        'TOTAL_NET_SALES': 'sum',\n",
    "        'TOTAL_BILLING_QTY': 'sum'\n",
    "    }).reset_index()\n",
    "    \n",
    "    # Convert to TimeGPT format\n",
    "    timegpt_data = daily_sales.rename(columns={\n",
    "        'BILLING_DATE': 'ds',\n",
    "        'TOTAL_NET_SALES': 'y',\n",
    "        'PRODUCT_KEY': 'unique_id'\n",
    "    })\n",
    "    \n",
    "    # Ensure proper data types\n",
    "    timegpt_data['ds'] = pd.to_datetime(timegpt_data['ds'])\n",
    "    timegpt_data['y'] = pd.to_numeric(timegpt_data['y'], errors='coerce')\n",
    "    \n",
    "    # Remove any rows with null sales values\n",
    "    timegpt_data = timegpt_data.dropna(subset=['y'])\n",
    "    \n",
    "    print(f\"   âœ… Data prepared: {len(timegpt_data)} rows, {timegpt_data['unique_id'].nunique()} products\")\n",
    "    \n",
    "    return timegpt_data\n",
    "\n",
    "def fix_frequency_issues_complete(data):\n",
    "    \"\"\"\n",
    "    Comprehensive frequency issues fix\n",
    "    \"\"\"\n",
    "    print(\"   â€¢ Diagnosing frequency issues...\")\n",
    "    \n",
    "    cleaned_data = []\n",
    "    successful_products = []\n",
    "    \n",
    "    for product_id, group in data.groupby('unique_id'):\n",
    "        # Sort and remove duplicates\n",
    "        group = group.sort_values('ds').drop_duplicates(subset=['ds'], keep='first')\n",
    "        \n",
    "        # Skip products with insufficient data\n",
    "        if len(group) < 30:\n",
    "            continue\n",
    "            \n",
    "        # Create complete date range\n",
    "        date_range = pd.date_range(\n",
    "            start=group['ds'].min(),\n",
    "            end=group['ds'].max(),\n",
    "            freq='D'\n",
    "        )\n",
    "        \n",
    "        # Create complete time series\n",
    "        complete_ts = pd.DataFrame({\n",
    "            'ds': date_range,\n",
    "            'unique_id': product_id\n",
    "        })\n",
    "        \n",
    "        # Merge with actual data\n",
    "        merged = pd.merge(complete_ts, group, on=['ds', 'unique_id'], how='left')\n",
    "        \n",
    "        # Fill missing values with interpolation\n",
    "        merged['y'] = merged['y'].interpolate(method='linear').fillna(0)\n",
    "        merged['y'] = merged['y'].clip(lower=0)  # Ensure no negative sales\n",
    "        \n",
    "        # Verify frequency can be inferred\n",
    "        if pd.infer_freq(merged['ds']) == 'D':\n",
    "            cleaned_data.append(merged)\n",
    "            successful_products.append(product_id)\n",
    "    \n",
    "    if cleaned_data:\n",
    "        final_data = pd.concat(cleaned_data, ignore_index=True)\n",
    "        print(f\"   âœ… Fixed frequency for {len(successful_products)} products\")\n",
    "        print(f\"   ðŸ“Š Final dataset: {len(final_data)} rows\")\n",
    "        return final_data\n",
    "    else:\n",
    "        raise ValueError(\"No products could be frequency-corrected\")\n",
    "\n",
    "def create_train_test_split(data, test_days=14):\n",
    "    \"\"\"\n",
    "    Create train-test split\n",
    "    \"\"\"\n",
    "    print(f\"   â€¢ Splitting data with {test_days} test days...\")\n",
    "    \n",
    "    # Split data by taking last N days for testing\n",
    "    test_data = data.groupby(\"unique_id\").tail(test_days)\n",
    "    train_data = (data.groupby(\"unique_id\")\n",
    "                 .apply(lambda group: group.iloc[:-test_days])\n",
    "                 .reset_index(drop=True))\n",
    "    \n",
    "    print(f\"   âœ… Train: {len(train_data)} rows, Test: {len(test_data)} rows\")\n",
    "    \n",
    "    return train_data, test_data\n",
    "\n",
    "def run_timegpt_forecasting(train_data, test_periods=7):\n",
    "    \"\"\"\n",
    "    Run TimeGPT forecasting\n",
    "    \"\"\"\n",
    "    print(f\"   â€¢ Running TimeGPT forecast for {test_periods} periods...\")\n",
    "    \n",
    "    try:\n",
    "        forecasts = timegpt.forecast(\n",
    "            df=train_data,\n",
    "            h=min(test_periods , 14),\n",
    "            freq='D',\n",
    "            level=[80, 90],  # Confidence intervals\n",
    "            time_col='ds',\n",
    "            target_col='y',\n",
    "            id_col='unique_id'\n",
    "        )\n",
    "        \n",
    "        print(f\"   âœ… Forecasts generated: {len(forecasts)} rows\")\n",
    "        return forecasts\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"   âŒ TimeGPT error: {e}\")\n",
    "        \n",
    "        # Fallback: Try with fewer products\n",
    "        print(\"   ðŸ”„ Trying with top 5 products...\")\n",
    "        top_5_products = train_data['unique_id'].value_counts().head(5).index.tolist()\n",
    "        small_train = train_data[train_data['unique_id'].isin(top_5_products)]\n",
    "        \n",
    "        forecasts = timegpt.forecast(\n",
    "            df=small_train,\n",
    "            X_df = x_data,\n",
    "            h=test_periods,\n",
    "            freq='D',\n",
    "            level=[80, 90],\n",
    "            time_col='ds',\n",
    "            target_col='y',\n",
    "            id_col='unique_id'\n",
    "        )\n",
    "        \n",
    "        print(f\"   âœ… Fallback forecasts generated: {len(forecasts)} rows\")\n",
    "        return forecasts\n",
    "\n",
    "def evaluate_forecasts(forecasts, test_data):\n",
    "    \"\"\"\n",
    "    Evaluate forecast performance with improved MAPE handling\n",
    "    \"\"\"\n",
    "    print(\"   â€¢ Calculating performance metrics...\")\n",
    "    \n",
    "    # Fix datetime types\n",
    "    forecasts['ds'] = pd.to_datetime(forecasts['ds'])\n",
    "    test_data['ds'] = pd.to_datetime(test_data['ds'])\n",
    "    \n",
    "    # Merge forecasts with test data\n",
    "    evaluation_data = pd.merge(\n",
    "        test_data,\n",
    "        forecasts[['ds', 'unique_id', 'TimeGPT']],\n",
    "        on=['ds', 'unique_id'],\n",
    "        how='inner'\n",
    "    )\n",
    "    \n",
    "    if len(evaluation_data) == 0:\n",
    "        print(\"   âŒ No matching data for evaluation\")\n",
    "        return None\n",
    "    \n",
    "    # Check for zero values in actual data\n",
    "    zero_count = (evaluation_data['y'] == 0).sum()\n",
    "    near_zero_count = (np.abs(evaluation_data['y']) < 1e-8).sum()\n",
    "    \n",
    "    print(f\"   ðŸ“Š Data quality check:\")\n",
    "    print(f\"      â€¢ Zero values in actual data: {zero_count}\")\n",
    "    print(f\"      â€¢ Near-zero values: {near_zero_count}\")\n",
    "    \n",
    "    # Calculate metrics by product\n",
    "    product_metrics = []\n",
    "    \n",
    "    for product_id in evaluation_data['unique_id'].unique():\n",
    "        product_eval = evaluation_data[evaluation_data['unique_id'] == product_id]\n",
    "        \n",
    "        if len(product_eval) > 0:\n",
    "            # Standard metrics\n",
    "            mse = mean_squared_error(product_eval['y'], product_eval['TimeGPT'])\n",
    "            mae = mean_absolute_error(product_eval['y'], product_eval['TimeGPT'])\n",
    "            rmse = np.sqrt(mse)\n",
    "            \n",
    "            # Improved MAPE calculation with zero handling\n",
    "            actual_values = product_eval['y'].values\n",
    "            predicted_values = product_eval['TimeGPT'].values\n",
    "            \n",
    "            # Method 1: Exclude zero values from MAPE calculation\n",
    "            non_zero_mask = actual_values != 0\n",
    "            if non_zero_mask.sum() > 0:\n",
    "                mape = np.mean(np.abs((actual_values[non_zero_mask] - predicted_values[non_zero_mask]) / actual_values[non_zero_mask])) * 100\n",
    "                effective_points = non_zero_mask.sum()\n",
    "            else:\n",
    "                mape = np.inf  # Still infinity if all values are zero\n",
    "                effective_points = 0\n",
    "            \n",
    "            # Alternative: sMAPE (Symmetric MAPE) - more robust to zeros\n",
    "            smape = np.mean(2 * np.abs(actual_values - predicted_values) / \n",
    "                           (np.abs(actual_values) + np.abs(predicted_values))) * 100\n",
    "            \n",
    "            # Alternative: WAPE (Weighted Absolute Percentage Error)\n",
    "            if actual_values.sum() != 0:\n",
    "                wape = np.sum(np.abs(actual_values - predicted_values)) / np.sum(np.abs(actual_values)) * 100\n",
    "            else:\n",
    "                wape = np.inf\n",
    "            \n",
    "            product_metrics.append({\n",
    "                'product': product_id,\n",
    "                'mse': mse,\n",
    "                'mae': mae,\n",
    "                'rmse': rmse,\n",
    "                'mape': mape,\n",
    "                'smape': smape,  # Symmetric MAPE - better for zeros\n",
    "                'wape': wape,    # Weighted APE - alternative metric\n",
    "                'data_points': len(product_eval),\n",
    "                'effective_points': effective_points,  # Points used in MAPE calculation\n",
    "                'zero_values': (actual_values == 0).sum()\n",
    "            })\n",
    "    \n",
    "    metrics_df = pd.DataFrame(product_metrics)\n",
    "    \n",
    "    # Calculate overall statistics using alternative metrics when MAPE is problematic\n",
    "    finite_mape_mask = np.isfinite(metrics_df['mape'])\n",
    "    \n",
    "    if finite_mape_mask.sum() > 0:\n",
    "        avg_mape = metrics_df.loc[finite_mape_mask, 'mape'].mean()\n",
    "        mape_note = f\"(calculated from {finite_mape_mask.sum()}/{len(metrics_df)} products)\"\n",
    "    else:\n",
    "        avg_mape = np.inf\n",
    "        mape_note = \"(all products have zero values - use sMAPE instead)\"\n",
    "    \n",
    "    overall_stats = {\n",
    "        'avg_mse': metrics_df['mse'].mean(),\n",
    "        'avg_mae': metrics_df['mae'].mean(),\n",
    "        'avg_rmse': metrics_df['rmse'].mean(),\n",
    "        'avg_mape': avg_mape,\n",
    "        'avg_smape': metrics_df['smape'].mean(),  # More reliable alternative\n",
    "        'avg_wape': metrics_df[np.isfinite(metrics_df['wape'])]['wape'].mean(),\n",
    "        'total_products': len(metrics_df),\n",
    "        'total_points': len(evaluation_data),\n",
    "        'products_with_zeros': (metrics_df['zero_values'] > 0).sum()\n",
    "    }\n",
    "    \n",
    "    print(f\"   âœ… Evaluation complete: {len(metrics_df)} products evaluated\")\n",
    "    print(f\"   ðŸ“Š Average MAE: {overall_stats['avg_mae']:.2f}\")\n",
    "    print(f\"   ðŸ“Š Average RMSE: {overall_stats['avg_rmse']:.2f}\")\n",
    "    print(f\"   ðŸ“Š Average sMAPE: {overall_stats['avg_smape']:.2f}% (recommended)\")\n",
    "    print(f\"   ðŸ“Š Average MAPE: {overall_stats['avg_mape']:.2f}% {mape_note}\")\n",
    "    print(f\"   âš ï¸  Products with zero values: {overall_stats['products_with_zeros']}\")\n",
    "    \n",
    "    return {\n",
    "        'product_metrics': metrics_df,\n",
    "        'overall_stats': overall_stats,\n",
    "        'evaluation_data': evaluation_data\n",
    "    }\n",
    "\n",
    "\n",
    "def create_comprehensive_visualizations(full_data, forecasts, test_data, evaluation_results):\n",
    "    \"\"\"\n",
    "    Create comprehensive visualizations using TimeGPT's built-in plotting\n",
    "    \"\"\"\n",
    "    print(\"   â€¢ Creating TimeGPT visualization...\")\n",
    "    \n",
    "    if evaluation_results is None:\n",
    "        print(\"   âŒ No evaluation data for visualization\")\n",
    "        return\n",
    "    \n",
    "    # Use TimeGPT's built-in plotting function\n",
    "    try:\n",
    "        # Plot using TimeGPT's native plotting with zoomed view\n",
    "        timegpt.plot(\n",
    "            test_data,\n",
    "            forecasts,\n",
    "            models=[\"TimeGPT\"],\n",
    "            level=[90],\n",
    "            time_col=\"ds\",\n",
    "            target_col=\"y\",\n",
    "            id_col=\"unique_id\",\n",
    "            max_insample_length=60  # Show last 60 days for context\n",
    "        )\n",
    "        \n",
    "        print(\"   âœ… TimeGPT native plot created\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"   âš ï¸ TimeGPT plot failed: {e}\")\n",
    "        print(\"   ðŸ”„ Creating custom matplotlib visualization...\")\n",
    "        \n",
    "        # Fallback to custom matplotlib plot\n",
    "        create_custom_visualization(full_data, forecasts, test_data, evaluation_results)\n",
    "\n",
    "def create_custom_visualization(full_data, forecasts, test_data, evaluation_results):\n",
    "    \"\"\"\n",
    "    Create custom matplotlib visualization\n",
    "    \"\"\"\n",
    "    evaluation_data = evaluation_results['evaluation_data']\n",
    "    product_metrics = evaluation_results['product_metrics']\n",
    "    \n",
    "    # Get top 3 products for plotting\n",
    "    top_products = product_metrics.nsmallest(3, 'mape')['product'].tolist()\n",
    "    \n",
    "    fig, axes = plt.subplots(len(top_products), 1, figsize=(15, 5*len(top_products)))\n",
    "    if len(top_products) == 1:\n",
    "        axes = [axes]\n",
    "    \n",
    "    for idx, product_id in enumerate(top_products):\n",
    "        # Get data for this product\n",
    "        product_train = full_data[full_data['unique_id'] == product_id]\n",
    "        product_test = evaluation_data[evaluation_data['unique_id'] == product_id]\n",
    "        product_pred = forecasts[forecasts['unique_id'] == product_id]\n",
    "        \n",
    "        # Get metrics\n",
    "        metrics = product_metrics[product_metrics['product'] == product_id].iloc[0]\n",
    "        \n",
    "        # Find split point\n",
    "        if len(product_test) > 0:\n",
    "            test_start = product_test['ds'].min()\n",
    "            train_plot = product_train[product_train['ds'] >= test_start - pd.Timedelta(days=30)]\n",
    "        \n",
    "        # Plot training data (last 30 days)\n",
    "        axes[idx].plot(train_plot['ds'], train_plot['y'], 'b-', linewidth=2, \n",
    "                      label='Training Data', alpha=0.7)\n",
    "        \n",
    "        # Plot actual test data\n",
    "        axes[idx].plot(product_test['ds'], product_test['y'], 'go-', linewidth=3, \n",
    "                      markersize=8, label='Actual Test Data')\n",
    "        \n",
    "        # Plot predictions\n",
    "        axes[idx].plot(product_pred['ds'], product_pred['TimeGPT'], 'r--', linewidth=3, \n",
    "                      marker='s', markersize=8, label='TimeGPT Predictions')\n",
    "        \n",
    "        # Add confidence intervals if available\n",
    "        if 'TimeGPT-lo-90' in product_pred.columns:\n",
    "            axes[idx].fill_between(product_pred['ds'], \n",
    "                                 product_pred['TimeGPT-lo-90'], \n",
    "                                 product_pred['TimeGPT-hi-90'], \n",
    "                                 color='red', alpha=0.2, label='90% Confidence')\n",
    "        \n",
    "        # Formatting\n",
    "        title = f'Product: {product_id[:50]}...\\n'\n",
    "        title += f'MAPE: {metrics[\"mape\"]:.1f}% | MAE: {metrics[\"mae\"]:.1f}'\n",
    "        \n",
    "        axes[idx].set_title(title, fontsize=12, fontweight='bold')\n",
    "        axes[idx].set_xlabel('Date')\n",
    "        axes[idx].set_ylabel('Sales')\n",
    "        axes[idx].legend()\n",
    "        axes[idx].grid(True, alpha=0.3)\n",
    "        \n",
    "        # Format dates\n",
    "        axes[idx].xaxis.set_major_formatter(mdates.DateFormatter('%m/%d'))\n",
    "        plt.setp(axes[idx].xaxis.get_majorticklabels(), rotation=45)\n",
    "    \n",
    "    plt.suptitle('ðŸ¤– TimeGPT Forecasting Results', fontsize=16, fontweight='bold')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"   âœ… Custom visualization created\")\n",
    "\n",
    "def generate_future_forecasts(data, horizon=14):\n",
    "    \"\"\"\n",
    "    Generate future forecasts\n",
    "    \"\"\"\n",
    "    print(f\"   â€¢ Generating {horizon}-day future forecasts...\")\n",
    "    \n",
    "    try:\n",
    "        future_forecasts = timegpt.forecast(\n",
    "            df=data,\n",
    "            h=horizon,\n",
    "            freq='D',\n",
    "            level=[80, 90],\n",
    "            time_col='ds',\n",
    "            target_col='y',\n",
    "            id_col='unique_id'\n",
    "        )\n",
    "        \n",
    "        # Create summary\n",
    "        forecast_summary = future_forecasts.groupby('unique_id').agg({\n",
    "            'TimeGPT': ['mean', 'sum']\n",
    "        }).round(2)\n",
    "        \n",
    "        forecast_summary.columns = ['avg_daily', 'total_forecast']\n",
    "        forecast_summary = forecast_summary.sort_values('total_forecast', ascending=False)\n",
    "        \n",
    "        print(f\"   âœ… Future forecasts generated\")\n",
    "        print(f\"   ðŸ“Š Top 5 products by projected sales:\")\n",
    "        print(forecast_summary.head())\n",
    "        \n",
    "        return future_forecasts\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"   âŒ Future forecast error: {e}\")\n",
    "        return None\n",
    "\n",
    "# Main execution function\n",
    "def run_complete_pipeline(df_sorted):\n",
    "    \"\"\"\n",
    "    Run the complete TimeGPT pipeline\n",
    "    \"\"\"\n",
    "    try:\n",
    "        results = complete_timegpt_pipeline(df_sorted, top_n_products=10)\n",
    "        \n",
    "        print(\"\\nðŸŽ‰ PIPELINE COMPLETED SUCCESSFULLY!\")\n",
    "        print(\"=\" * 60)\n",
    "        print(\"ðŸ“‹ Results Summary:\")\n",
    "        print(f\"   â€¢ Products processed: {results['cleaned_data']['unique_id'].nunique()}\")\n",
    "        print(f\"   â€¢ Training data points: {len(results['train_data'])}\")\n",
    "        print(f\"   â€¢ Test data points: {len(results['test_data'])}\")\n",
    "        \n",
    "        if results['evaluation']:\n",
    "            print(f\"   â€¢ Average MAPE: {results['evaluation']['overall_stats']['avg_mape']:.2f}%\")\n",
    "        \n",
    "        return results\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Pipeline failed: {e}\")\n",
    "        return None\n",
    "\n",
    "results = run_complete_pipeline(df_sorted)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'results' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mresults\u001b[49m:\n\u001b[32m      2\u001b[39m     cleaned_data = results[\u001b[33m'\u001b[39m\u001b[33mcleaned_data\u001b[39m\u001b[33m'\u001b[39m]\n\u001b[32m      3\u001b[39m     forecasts = results[\u001b[33m'\u001b[39m\u001b[33mforecasts\u001b[39m\u001b[33m'\u001b[39m]\n",
      "\u001b[31mNameError\u001b[39m: name 'results' is not defined"
     ]
    }
   ],
   "source": [
    "if results:\n",
    "    cleaned_data = results['cleaned_data']\n",
    "    forecasts = results['forecasts']\n",
    "    evaluation = results['evaluation']\n",
    "    future_forecasts = results['future_forecasts']\n",
    "    \n",
    "    # Print performance summary\n",
    "    if evaluation:\n",
    "        print(f\"Model Performance: {evaluation['overall_stats']['avg_mape']:.1f}% MAPE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "agent_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
