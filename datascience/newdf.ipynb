{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nixtlats in /opt/anaconda3/envs/agent_env/lib/python3.11/site-packages (0.5.2)\n",
      "Requirement already satisfied: httpx in /opt/anaconda3/envs/agent_env/lib/python3.11/site-packages (from nixtlats) (0.28.1)\n",
      "Requirement already satisfied: pandas in /opt/anaconda3/envs/agent_env/lib/python3.11/site-packages (from nixtlats) (2.3.0)\n",
      "Requirement already satisfied: pydantic in /opt/anaconda3/envs/agent_env/lib/python3.11/site-packages (from nixtlats) (2.11.7)\n",
      "Requirement already satisfied: requests in /opt/anaconda3/envs/agent_env/lib/python3.11/site-packages (from nixtlats) (2.32.4)\n",
      "Requirement already satisfied: tenacity in /opt/anaconda3/envs/agent_env/lib/python3.11/site-packages (from nixtlats) (9.1.2)\n",
      "Requirement already satisfied: utilsforecast>=0.1.7 in /opt/anaconda3/envs/agent_env/lib/python3.11/site-packages (from nixtlats) (0.2.12)\n",
      "Requirement already satisfied: numpy in /opt/anaconda3/envs/agent_env/lib/python3.11/site-packages (from utilsforecast>=0.1.7->nixtlats) (2.3.0)\n",
      "Requirement already satisfied: packaging in /opt/anaconda3/envs/agent_env/lib/python3.11/site-packages (from utilsforecast>=0.1.7->nixtlats) (25.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/anaconda3/envs/agent_env/lib/python3.11/site-packages (from pandas->nixtlats) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/anaconda3/envs/agent_env/lib/python3.11/site-packages (from pandas->nixtlats) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/anaconda3/envs/agent_env/lib/python3.11/site-packages (from pandas->nixtlats) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in /opt/anaconda3/envs/agent_env/lib/python3.11/site-packages (from python-dateutil>=2.8.2->pandas->nixtlats) (1.17.0)\n",
      "Requirement already satisfied: anyio in /opt/anaconda3/envs/agent_env/lib/python3.11/site-packages (from httpx->nixtlats) (4.9.0)\n",
      "Requirement already satisfied: certifi in /opt/anaconda3/envs/agent_env/lib/python3.11/site-packages (from httpx->nixtlats) (2025.6.15)\n",
      "Requirement already satisfied: httpcore==1.* in /opt/anaconda3/envs/agent_env/lib/python3.11/site-packages (from httpx->nixtlats) (1.0.9)\n",
      "Requirement already satisfied: idna in /opt/anaconda3/envs/agent_env/lib/python3.11/site-packages (from httpx->nixtlats) (3.10)\n",
      "Requirement already satisfied: h11>=0.16 in /opt/anaconda3/envs/agent_env/lib/python3.11/site-packages (from httpcore==1.*->httpx->nixtlats) (0.16.0)\n",
      "Requirement already satisfied: sniffio>=1.1 in /opt/anaconda3/envs/agent_env/lib/python3.11/site-packages (from anyio->httpx->nixtlats) (1.3.1)\n",
      "Requirement already satisfied: typing_extensions>=4.5 in /opt/anaconda3/envs/agent_env/lib/python3.11/site-packages (from anyio->httpx->nixtlats) (4.14.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /opt/anaconda3/envs/agent_env/lib/python3.11/site-packages (from pydantic->nixtlats) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in /opt/anaconda3/envs/agent_env/lib/python3.11/site-packages (from pydantic->nixtlats) (2.33.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in /opt/anaconda3/envs/agent_env/lib/python3.11/site-packages (from pydantic->nixtlats) (0.4.1)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /opt/anaconda3/envs/agent_env/lib/python3.11/site-packages (from requests->nixtlats) (3.4.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/anaconda3/envs/agent_env/lib/python3.11/site-packages (from requests->nixtlats) (2.4.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install nixtlats\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0.4</th>\n",
       "      <th>Unnamed: 0.3</th>\n",
       "      <th>Unnamed: 0.2</th>\n",
       "      <th>Unnamed: 0.1</th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>BILLING_DATE</th>\n",
       "      <th>SITE</th>\n",
       "      <th>BRAND</th>\n",
       "      <th>FORMAT_DESC</th>\n",
       "      <th>MH_SEGMENT</th>\n",
       "      <th>MH_FAMILY</th>\n",
       "      <th>MH_CLASS</th>\n",
       "      <th>MH_BRICK</th>\n",
       "      <th>ARTICLE</th>\n",
       "      <th>TOTAL_BILLING_QTY</th>\n",
       "      <th>TOTAL_GROSS_MARGIN</th>\n",
       "      <th>TOTAL_MARKDOWN</th>\n",
       "      <th>TOTAL_NET_SALES</th>\n",
       "      <th>TOTAL_GROSS_SALES</th>\n",
       "      <th>TOTAL_DISCOUNT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>712337</td>\n",
       "      <td>712337</td>\n",
       "      <td>712337</td>\n",
       "      <td>712337</td>\n",
       "      <td>712337</td>\n",
       "      <td>2022-09-30</td>\n",
       "      <td>8013</td>\n",
       "      <td>04017</td>\n",
       "      <td>TRENDS</td>\n",
       "      <td>WOMENS WEAR</td>\n",
       "      <td>WESTERN WEAR</td>\n",
       "      <td>WINTER WEAR</td>\n",
       "      <td>SWEATSHIRTS</td>\n",
       "      <td>441146771010</td>\n",
       "      <td>1</td>\n",
       "      <td>78.66</td>\n",
       "      <td>0.0</td>\n",
       "      <td>334.28</td>\n",
       "      <td>499.0</td>\n",
       "      <td>164.72</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>669408</td>\n",
       "      <td>669408</td>\n",
       "      <td>669408</td>\n",
       "      <td>669408</td>\n",
       "      <td>669408</td>\n",
       "      <td>2022-09-30</td>\n",
       "      <td>8013</td>\n",
       "      <td>D525</td>\n",
       "      <td>TRENDS</td>\n",
       "      <td>WOMENS WEAR</td>\n",
       "      <td>WESTERN WEAR</td>\n",
       "      <td>CASUAL WEAR</td>\n",
       "      <td>T SHIRTS</td>\n",
       "      <td>441137882004</td>\n",
       "      <td>1</td>\n",
       "      <td>71.91</td>\n",
       "      <td>0.0</td>\n",
       "      <td>240.26</td>\n",
       "      <td>399.0</td>\n",
       "      <td>158.74</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>236195</td>\n",
       "      <td>236195</td>\n",
       "      <td>236195</td>\n",
       "      <td>236195</td>\n",
       "      <td>236195</td>\n",
       "      <td>2022-09-30</td>\n",
       "      <td>8013</td>\n",
       "      <td>09888</td>\n",
       "      <td>TRENDS</td>\n",
       "      <td>MENS CASUAL</td>\n",
       "      <td>ACTIVE WEAR</td>\n",
       "      <td>TOPS</td>\n",
       "      <td>T SHIRTS</td>\n",
       "      <td>441134761003</td>\n",
       "      <td>1</td>\n",
       "      <td>88.72</td>\n",
       "      <td>0.0</td>\n",
       "      <td>322.58</td>\n",
       "      <td>599.0</td>\n",
       "      <td>276.42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>298014</td>\n",
       "      <td>298014</td>\n",
       "      <td>298014</td>\n",
       "      <td>298014</td>\n",
       "      <td>298014</td>\n",
       "      <td>2022-09-30</td>\n",
       "      <td>8013</td>\n",
       "      <td>04063</td>\n",
       "      <td>TRENDS</td>\n",
       "      <td>MENS CASUAL</td>\n",
       "      <td>ACTIVE WEAR</td>\n",
       "      <td>TOPS</td>\n",
       "      <td>T SHIRTS</td>\n",
       "      <td>441145712004</td>\n",
       "      <td>1</td>\n",
       "      <td>408.46</td>\n",
       "      <td>0.0</td>\n",
       "      <td>799.00</td>\n",
       "      <td>799.0</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>776725</td>\n",
       "      <td>776725</td>\n",
       "      <td>776725</td>\n",
       "      <td>776725</td>\n",
       "      <td>776725</td>\n",
       "      <td>2022-09-30</td>\n",
       "      <td>8013</td>\n",
       "      <td>04042</td>\n",
       "      <td>TRENDS</td>\n",
       "      <td>MENS WEAR</td>\n",
       "      <td>SMART CASUALS</td>\n",
       "      <td>BOTTOMS</td>\n",
       "      <td>TROUSERS</td>\n",
       "      <td>441138083006</td>\n",
       "      <td>1</td>\n",
       "      <td>239.82</td>\n",
       "      <td>0.0</td>\n",
       "      <td>790.98</td>\n",
       "      <td>999.0</td>\n",
       "      <td>208.02</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0.4  Unnamed: 0.3  Unnamed: 0.2  Unnamed: 0.1  Unnamed: 0  \\\n",
       "0        712337        712337        712337        712337      712337   \n",
       "1        669408        669408        669408        669408      669408   \n",
       "2        236195        236195        236195        236195      236195   \n",
       "3        298014        298014        298014        298014      298014   \n",
       "4        776725        776725        776725        776725      776725   \n",
       "\n",
       "  BILLING_DATE  SITE  BRAND FORMAT_DESC   MH_SEGMENT      MH_FAMILY  \\\n",
       "0   2022-09-30  8013  04017      TRENDS  WOMENS WEAR   WESTERN WEAR   \n",
       "1   2022-09-30  8013   D525      TRENDS  WOMENS WEAR   WESTERN WEAR   \n",
       "2   2022-09-30  8013  09888      TRENDS  MENS CASUAL    ACTIVE WEAR   \n",
       "3   2022-09-30  8013  04063      TRENDS  MENS CASUAL    ACTIVE WEAR   \n",
       "4   2022-09-30  8013  04042      TRENDS    MENS WEAR  SMART CASUALS   \n",
       "\n",
       "      MH_CLASS     MH_BRICK       ARTICLE  TOTAL_BILLING_QTY  \\\n",
       "0  WINTER WEAR  SWEATSHIRTS  441146771010                  1   \n",
       "1  CASUAL WEAR     T SHIRTS  441137882004                  1   \n",
       "2         TOPS     T SHIRTS  441134761003                  1   \n",
       "3         TOPS     T SHIRTS  441145712004                  1   \n",
       "4      BOTTOMS     TROUSERS  441138083006                  1   \n",
       "\n",
       "   TOTAL_GROSS_MARGIN  TOTAL_MARKDOWN  TOTAL_NET_SALES  TOTAL_GROSS_SALES  \\\n",
       "0               78.66             0.0           334.28              499.0   \n",
       "1               71.91             0.0           240.26              399.0   \n",
       "2               88.72             0.0           322.58              599.0   \n",
       "3              408.46             0.0           799.00              799.0   \n",
       "4              239.82             0.0           790.98              999.0   \n",
       "\n",
       "   TOTAL_DISCOUNT  \n",
       "0          164.72  \n",
       "1          158.74  \n",
       "2          276.42  \n",
       "3            0.00  \n",
       "4          208.02  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "df = pd.read_csv(\"data-8013-trends.csv\")\n",
    "df_sorted = df.sort_values(by='BILLING_DATE')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 STARTING COMPLETE TIMEGPT PIPELINE WITH EXOGENOUS FEATURES\n",
      "======================================================================\n",
      "\n",
      "📋 STEP 1: DATA CLEANING WITH EXOGENOUS FEATURES\n",
      "   • Creating exogenous features before data aggregation...\n",
      "   • Creating exogenous features...\n",
      "   • Created 14 predictable exogenous features\n",
      "   • Found 12853 negative sales records\n",
      "   📊 Product filtering with exogenous features:\n",
      "      • Viable for forecasting: 381\n",
      "      • Selected top: 8\n",
      "   • Aggregating with enhanced exogenous variables...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:nixtlats.nixtla_client:Validating inputs...\n",
      "INFO:nixtlats.nixtla_client:Preprocessing dataframes...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ✅ Enhanced data with exogenous features: 6,245 rows, 8 products\n",
      "   📊 Exogenous features included: 14\n",
      "\n",
      "🔧 STEP 2: FIXING FREQUENCY ISSUES\n",
      "   • Applying robust frequency correction for irregular data...\n",
      "      Processing: 04042 | MENS WEAR | SMART CASUALS | BOTT... (790 records)\n",
      "        📊 Missing ratio: 0.25%\n",
      "        ✅ Success: 2 gaps filled\n",
      "      Processing: 04042 | MENS WEAR | SMART CASUALS | TOPS... (792 records)\n",
      "        📊 Missing ratio: 0.00%\n",
      "        ✅ Success: 0 gaps filled\n",
      "      Processing: 04063 | MENS CASUAL | ACTIVE WEAR | BOTT... (791 records)\n",
      "        📊 Missing ratio: 0.13%\n",
      "        ✅ Success: 1 gaps filled\n",
      "      Processing: 04063 | MENS CASUAL | ACTIVE WEAR | TOPS... (792 records)\n",
      "        📊 Missing ratio: 0.00%\n",
      "        ✅ Success: 0 gaps filled\n",
      "      Processing: D699 | WOMENS WEAR | ETHNIC WEAR | TOPWE... (788 records)\n",
      "        📊 Missing ratio: 0.51%\n",
      "        ✅ Success: 4 gaps filled\n",
      "      Processing: D699 | WOMENS WEAR | ETHNIC WEAR | TOPWE... (709 records)\n",
      "        📊 Missing ratio: 10.37%\n",
      "        ✅ Success: 82 gaps filled\n",
      "      Processing: F085 | WOMENS WEAR | ETHNIC WEAR | TOPWE... (792 records)\n",
      "        📊 Missing ratio: 0.00%\n",
      "        ✅ Success: 0 gaps filled\n",
      "      Processing: F639 | WOMENS WEAR | ETHNIC WEAR | TOPWE... (791 records)\n",
      "        📊 Missing ratio: 0.13%\n",
      "        ✅ Success: 1 gaps filled\n",
      "   ✅ Robust processing complete: 8 products\n",
      "   📊 Final dataset: 6,335 rows\n",
      "\n",
      "✂️ STEP 3: TRAIN-TEST SPLIT\n",
      "   • Splitting data with 21 test days...\n",
      "   ✅ Train: 6167 rows, Test: 168 rows\n",
      "\n",
      "🤖 STEP 4: TIMEGPT FORECASTING WITH EXOGENOUS FEATURES\n",
      "   🎯 Running TimeGPT forecast for 7 periods...\n",
      "   • Using exogenous variables: ['year', 'month', 'day_of_week', 'is_weekend', 'is_month_start', 'is_month_end', 'is_quarter_start', 'is_quarter_end', 'quarter', 'is_republic_day', 'is_holi', 'is_diwali', 'is_independence_day', 'is_christmas']\n",
      "   • Main data shape: (6167, 17)\n",
      "   • Future exogenous data shape: (56, 16)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:nixtlats.nixtla_client:Using the following exogenous variables: year, month, day_of_week, is_weekend, is_month_start, is_month_end, is_quarter_start, is_quarter_end, quarter, is_republic_day, is_independence_day, is_christmas, is_holi, is_diwali\n",
      "INFO:nixtlats.nixtla_client:Calling Forecast Endpoint...\n",
      "INFO:nixtlats.nixtla_client:Validating inputs...\n",
      "INFO:nixtlats.nixtla_client:Preprocessing dataframes...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ✅ Forecasts generated: 56 rows\n",
      "\n",
      "📊 STEP 5: MODEL EVALUATION\n",
      "   • Calculating performance metrics...\n",
      "   ✅ Evaluation complete: 8 products evaluated\n",
      "   📊 Average MAE: 11952.41\n",
      "   📊 Average RMSE: 13744.33\n",
      "   📊 Average sMAPE: 100.08% (recommended)\n",
      "   📊 Average MAPE: 115.44% (calculated from 8/8 products)\n",
      "\n",
      "📈 STEP 6: CREATING VISUALIZATIONS\n",
      "   • Creating visualizations...\n",
      "   ✅ TimeGPT native plot created\n",
      "\n",
      "🔮 STEP 7: FUTURE FORECASTING\n",
      "   • Generating 14-day future forecasts...\n",
      "   • Including exogenous features: ['year', 'month', 'day_of_week', 'is_weekend', 'is_month_start', 'is_month_end', 'is_quarter_start', 'is_quarter_end', 'quarter', 'is_republic_day', 'is_holi', 'is_diwali', 'is_independence_day', 'is_christmas']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:nixtlats.nixtla_client:The specified horizon \"h\" exceeds the model horizon. This may lead to less accurate forecasts. Please consider using a smaller horizon.\n",
      "INFO:nixtlats.nixtla_client:Using the following exogenous variables: year, month, day_of_week, is_weekend, is_month_start, is_month_end, is_quarter_start, is_quarter_end, quarter, is_republic_day, is_independence_day, is_christmas, is_holi, is_diwali\n",
      "INFO:nixtlats.nixtla_client:Calling Forecast Endpoint...\n",
      "INFO:nixtlats.nixtla_client:Attempt 1 failed...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ✅ Future forecasts generated\n",
      "   📊 Top 5 products by projected sales:\n",
      "                                                    avg_daily  total_forecast\n",
      "unique_id                                                                    \n",
      "F085 | WOMENS WEAR | ETHNIC WEAR | TOPWEAR | KU...   94967.11      1329539.58\n",
      "04042 | MENS WEAR | SMART CASUALS | TOPS | SHIRTS    49429.55       692013.77\n",
      "04042 | MENS WEAR | SMART CASUALS | BOTTOMS | T...   20649.48       289092.73\n",
      "04063 | MENS CASUAL | ACTIVE WEAR | BOTTOMS | T...   20093.73       281312.17\n",
      "04063 | MENS CASUAL | ACTIVE WEAR | TOPS | T SH...   15752.59       220536.23\n",
      "\n",
      "🎉 PIPELINE COMPLETED SUCCESSFULLY!\n",
      "======================================================================\n",
      "📊 RESULTS SUMMARY:\n",
      "• Products forecasted: 8\n",
      "• Average MAE: 11952.41\n",
      "• Average sMAPE: 100.08%\n",
      "• Check the visualizations above for detailed results\n"
     ]
    }
   ],
   "source": [
    "# COMPLETE TIMEGPT IMPLEMENTATION WITH EXOGENOUS VARIABLES - FIXED VERSION\n",
    "# ========================================================================\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "from nixtlats import TimeGPT\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Initialize TimeGPT client with your token\n",
    "timegpt = TimeGPT(token='nixak-AI31YupjpWhin07kPGKOhvW5zj8IliWOlTSWLEInpOuKHNWNhtSETsXIFgyEiYT58g3Hk0hvMFhnJpdS')\n",
    "\n",
    "# ========================================================================\n",
    "# STEP 1: EXOGENOUS FEATURE CREATION\n",
    "# ========================================================================\n",
    "\n",
    "def create_calendar_features(df):\n",
    "    \"\"\"Create comprehensive calendar features for Indian retail\"\"\"\n",
    "    df = df.copy()\n",
    "    df['BILLING_DATE'] = pd.to_datetime(df['BILLING_DATE'])\n",
    "    \n",
    "    # Basic temporal features\n",
    "    df['year'] = df['BILLING_DATE'].dt.year\n",
    "    df['month'] = df['BILLING_DATE'].dt.month\n",
    "    df['day_of_week'] = df['BILLING_DATE'].dt.dayofweek\n",
    "    df['day_of_year'] = df['BILLING_DATE'].dt.dayofyear\n",
    "    df['week_of_year'] = df['BILLING_DATE'].dt.isocalendar().week\n",
    "    df['quarter'] = df['BILLING_DATE'].dt.quarter\n",
    "    \n",
    "    # Weekend indicator\n",
    "    df['is_weekend'] = (df['day_of_week'] >= 5).astype(int)\n",
    "    \n",
    "    # Month-end/start effects\n",
    "    df['is_month_start'] = df['BILLING_DATE'].dt.is_month_start.astype(int)\n",
    "    df['is_month_end'] = df['BILLING_DATE'].dt.is_month_end.astype(int)\n",
    "    df['is_quarter_start'] = df['BILLING_DATE'].dt.is_quarter_start.astype(int)\n",
    "    df['is_quarter_end'] = df['BILLING_DATE'].dt.is_quarter_end.astype(int)\n",
    "    \n",
    "    return df\n",
    "\n",
    "def create_indian_holiday_features(df):\n",
    "    \"\"\"Add Indian holiday indicators based on 2024 calendar\"\"\"\n",
    "    df = df.copy()\n",
    "    \n",
    "    # Major Indian holidays for 2024 (extend for other years)\n",
    "    major_holidays = {\n",
    "        '2024-01-26': 'republic_day',\n",
    "        '2024-03-25': 'holi', \n",
    "        '2024-04-11': 'eid_ul_fitr',\n",
    "        '2024-04-17': 'ram_navami',\n",
    "        '2024-08-15': 'independence_day',\n",
    "        '2024-08-26': 'janmashtami',\n",
    "        '2024-10-12': 'dussehra',\n",
    "        '2024-10-31': 'diwali',\n",
    "        '2024-12-25': 'christmas'\n",
    "    }\n",
    "    \n",
    "    # Create holiday indicators\n",
    "    for date_str, holiday_name in major_holidays.items():\n",
    "        df[f'is_{holiday_name}'] = (df['BILLING_DATE'].dt.strftime('%Y-%m-%d') == date_str).astype(int)\n",
    "    \n",
    "    # Holiday proximity features (corrected variable names)\n",
    "    diwali_date = pd.to_datetime('2024-10-31')\n",
    "    for days in [1, 2, 3]:\n",
    "        df[f'days_before_diwali_{days}'] = (df['BILLING_DATE'] == diwali_date - pd.Timedelta(days=days)).astype(int)\n",
    "        df[f'days_after_diwali_{days}'] = (df['BILLING_DATE'] == diwali_date + pd.Timedelta(days=days)).astype(int)\n",
    "    \n",
    "    return df\n",
    "\n",
    "def create_business_features(df):\n",
    "    \"\"\"Create features from existing business data\"\"\"\n",
    "    df = df.copy()\n",
    "    \n",
    "    # Categorical encoding for key business dimensions\n",
    "    categorical_cols = ['SITE', 'BRAND', 'FORMAT_DESC', 'MH_SEGMENT', 'MH_FAMILY']\n",
    "    \n",
    "    for col in categorical_cols:\n",
    "        if col in df.columns:\n",
    "            # Create frequency encoding\n",
    "            freq_map = df[col].value_counts(normalize=True).to_dict()\n",
    "            df[f'{col}_frequency'] = df[col].map(freq_map)\n",
    "            \n",
    "            # One-hot encode top categories only (to avoid high dimensionality)\n",
    "            top_categories = df[col].value_counts().head(5).index\n",
    "            for category in top_categories:\n",
    "                df[f'{col}_{category}'] = (df[col] == category).astype(int)\n",
    "    \n",
    "    # Price-related features (handle division by zero)\n",
    "    df['avg_selling_price'] = df['TOTAL_NET_SALES'] / df['TOTAL_BILLING_QTY'].replace(0, 1)\n",
    "    df['discount_rate'] = df['TOTAL_DISCOUNT'] / df['TOTAL_GROSS_SALES'].replace(0, 1)\n",
    "    df['margin_rate'] = df['TOTAL_GROSS_MARGIN'] / df['TOTAL_NET_SALES'].replace(0, 1)\n",
    "    \n",
    "    return df\n",
    "\n",
    "def prepare_exogenous_data_for_timegpt(df):\n",
    "    \"\"\"Prepare exogenous data in TimeGPT format\"\"\"\n",
    "    print(\"   • Creating exogenous features...\")\n",
    "    \n",
    "    # Step 1: Create all features\n",
    "    df_enhanced = create_calendar_features(df)\n",
    "    df_enhanced = create_indian_holiday_features(df_enhanced)\n",
    "    df_enhanced = create_business_features(df_enhanced)\n",
    "    \n",
    "    # Step 2: Define predictable features (can be calculated for future dates)\n",
    "    predictable_features = [\n",
    "        'year', 'month', 'day_of_week', 'is_weekend', 'is_month_start', \n",
    "        'is_month_end', 'is_quarter_start', 'is_quarter_end', 'quarter',\n",
    "        'is_republic_day', 'is_holi', 'is_diwali', 'is_independence_day',\n",
    "        'is_christmas'\n",
    "    ]\n",
    "    \n",
    "    # Filter to only include features that actually exist in the data\n",
    "    available_features = [f for f in predictable_features if f in df_enhanced.columns]\n",
    "    \n",
    "    print(f\"   • Created {len(available_features)} predictable exogenous features\")\n",
    "    \n",
    "    return df_enhanced, available_features\n",
    "\n",
    "# ========================================================================\n",
    "# STEP 2: DATA CLEANING AND PREPARATION\n",
    "# ========================================================================\n",
    "\n",
    "def clean_and_prepare_data_with_exogenous(df_sorted, top_n_products=8):\n",
    "    \"\"\"Enhanced data cleaning with exogenous feature integration\"\"\"\n",
    "    print(\"   • Creating exogenous features before data aggregation...\")\n",
    "    \n",
    "    # Step 1: Create exogenous features on raw data FIRST\n",
    "    df_enhanced, predictable_features = prepare_exogenous_data_for_timegpt(df_sorted)\n",
    "    \n",
    "    # Handle negative sales values\n",
    "    print(f\"   • Found {(df_enhanced['TOTAL_NET_SALES'] < 0).sum()} negative sales records\")\n",
    "    df_enhanced.loc[df_enhanced['TOTAL_NET_SALES'] < 0, 'TOTAL_NET_SALES'] = 0\n",
    "    \n",
    "    # Create PRODUCT_KEY if not exists\n",
    "    if 'PRODUCT_KEY' not in df_enhanced.columns:\n",
    "        df_enhanced['PRODUCT_KEY'] = (\n",
    "            df_enhanced['BRAND'].astype(str) + ' | ' + \n",
    "            df_enhanced['MH_SEGMENT'].astype(str) + ' | ' + \n",
    "            df_enhanced['MH_FAMILY'].astype(str) + ' | ' + \n",
    "            df_enhanced['MH_CLASS'].astype(str) + ' | ' + \n",
    "            df_enhanced['MH_BRICK'].astype(str)\n",
    "        )\n",
    "    \n",
    "    # Product filtering\n",
    "    product_analysis = df_enhanced.groupby('PRODUCT_KEY').agg({\n",
    "        'TOTAL_NET_SALES': ['sum', 'mean', 'count', 'std'],\n",
    "        'BILLING_DATE': ['min', 'max', 'nunique'],\n",
    "        'TOTAL_BILLING_QTY': 'sum',\n",
    "        'TOTAL_GROSS_MARGIN': 'sum'\n",
    "    }).round(2)\n",
    "    \n",
    "    product_analysis.columns = ['total_sales', 'avg_sales', 'transaction_count','sales_std', 'first_date', 'last_date', 'unique_days','total_qty', 'total_margin']\n",
    "    \n",
    "    # Calculate quality metrics\n",
    "    product_analysis['data_span_days'] = (product_analysis['last_date'] - product_analysis['first_date']).dt.days + 1\n",
    "    product_analysis['data_density'] = product_analysis['unique_days'] / product_analysis['data_span_days']\n",
    "    product_analysis['cv'] = product_analysis['sales_std'] / product_analysis['avg_sales'].replace(0, 1)\n",
    "    \n",
    "    # Product filtering criteria\n",
    "    viable_products = product_analysis[\n",
    "        (product_analysis['unique_days'] >= 60) &\n",
    "        (product_analysis['data_span_days'] >= 90) &\n",
    "        (product_analysis['data_density'] >= 0.3) &\n",
    "        (product_analysis['total_sales'] >= 10000) &\n",
    "        (product_analysis['transaction_count'] >= 30) &\n",
    "        (product_analysis['cv'] < 3)\n",
    "    ].sort_values('total_sales', ascending=False)\n",
    "    \n",
    "    print(f\"   📊 Product filtering with exogenous features:\")\n",
    "    print(f\"      • Viable for forecasting: {len(viable_products)}\")\n",
    "    print(f\"      • Selected top: {min(top_n_products, len(viable_products))}\")\n",
    "    \n",
    "    if len(viable_products) == 0:\n",
    "        viable_products = product_analysis[\n",
    "            (product_analysis['unique_days'] >= 30) &\n",
    "            (product_analysis['total_sales'] >= 5000)\n",
    "        ].sort_values('total_sales', ascending=False)\n",
    "    \n",
    "    selected_products = viable_products.head(min(top_n_products, len(viable_products))).index.tolist()\n",
    "    filtered_df = df_enhanced[df_enhanced['PRODUCT_KEY'].isin(selected_products)].copy()\n",
    "    \n",
    "    print(\"   • Aggregating with enhanced exogenous variables...\")\n",
    "    \n",
    "    # Aggregation dictionary\n",
    "    agg_dict = {\n",
    "        'TOTAL_NET_SALES': 'sum',\n",
    "        'TOTAL_BILLING_QTY': 'sum',\n",
    "        'TOTAL_GROSS_MARGIN': 'sum',\n",
    "        'TOTAL_GROSS_SALES': 'sum',\n",
    "        'TOTAL_DISCOUNT': 'sum'\n",
    "    }\n",
    "    \n",
    "    # Add predictable exogenous features to aggregation (using 'first' since they're constant per day)\n",
    "    for feature in predictable_features:\n",
    "        if feature in filtered_df.columns:\n",
    "            agg_dict[feature] = 'first'\n",
    "    \n",
    "    daily_sales = filtered_df.groupby(['BILLING_DATE', 'PRODUCT_KEY']).agg(agg_dict).reset_index()\n",
    "    \n",
    "    # Rename columns for TimeGPT\n",
    "    column_mapping = {\n",
    "        'BILLING_DATE': 'ds',\n",
    "        'TOTAL_NET_SALES': 'y',\n",
    "        'PRODUCT_KEY': 'unique_id',\n",
    "        'TOTAL_BILLING_QTY': 'qty',\n",
    "        'TOTAL_GROSS_MARGIN': 'margin',\n",
    "        'TOTAL_GROSS_SALES': 'gross_sales',\n",
    "        'TOTAL_DISCOUNT': 'discount'\n",
    "    }\n",
    "    \n",
    "    timegpt_data = daily_sales.rename(columns=column_mapping)\n",
    "    \n",
    "    # Data type optimization\n",
    "    timegpt_data['ds'] = pd.to_datetime(timegpt_data['ds'])\n",
    "    for col in ['y', 'qty', 'margin', 'gross_sales', 'discount']:\n",
    "        if col in timegpt_data.columns:\n",
    "            timegpt_data[col] = pd.to_numeric(timegpt_data[col], errors='coerce')\n",
    "    \n",
    "    # Remove invalid records\n",
    "    timegpt_data = timegpt_data.dropna(subset=['y'])\n",
    "    \n",
    "    print(f\"   ✅ Enhanced data with exogenous features: {len(timegpt_data):,} rows, {timegpt_data['unique_id'].nunique()} products\")\n",
    "    print(f\"   📊 Exogenous features included: {len([f for f in predictable_features if f in timegpt_data.columns])}\")\n",
    "    \n",
    "    return timegpt_data, predictable_features\n",
    "\n",
    "# ========================================================================\n",
    "# STEP 3: FREQUENCY FIXING\n",
    "# ========================================================================\n",
    "\n",
    "def fix_frequency_issues_complete(data):\n",
    "    \"\"\"Robust frequency fixing optimized for irregular data patterns\"\"\"\n",
    "    print(\"   • Applying robust frequency correction for irregular data...\")\n",
    "    \n",
    "    cleaned_data = []\n",
    "    successful_products = []\n",
    "    \n",
    "    for product_id, group in data.groupby('unique_id'):\n",
    "        # Sort and remove duplicates\n",
    "        group = group.sort_values('ds').drop_duplicates(subset=['ds'], keep='first')\n",
    "        \n",
    "        print(f\"      Processing: {product_id[:40]}... ({len(group)} records)\")\n",
    "        \n",
    "        # Skip products with insufficient data\n",
    "        if len(group) < 50:\n",
    "            print(f\"        ❌ Insufficient data: {len(group)} < 50 days\")\n",
    "            continue\n",
    "        \n",
    "        # Create complete date range\n",
    "        date_range = pd.date_range(\n",
    "            start=group['ds'].min(),\n",
    "            end=group['ds'].max(),\n",
    "            freq='D'\n",
    "        )\n",
    "        \n",
    "        expected_points = len(date_range)\n",
    "        missing_points = expected_points - len(group)\n",
    "        missing_ratio = missing_points / expected_points\n",
    "        \n",
    "        print(f\"        📊 Missing ratio: {missing_ratio:.2%}\")\n",
    "        \n",
    "        # Skip products with too much missing data\n",
    "        if missing_ratio > 0.7:\n",
    "            print(f\"        ❌ Too sparse: {missing_ratio:.1%} missing data\")\n",
    "            continue\n",
    "        \n",
    "        # Create complete time series\n",
    "        complete_ts = pd.DataFrame({\n",
    "            'ds': date_range,\n",
    "            'unique_id': product_id\n",
    "        })\n",
    "        \n",
    "        # Merge with actual data\n",
    "        merged = pd.merge(complete_ts, group, on=['ds', 'unique_id'], how='left')\n",
    "        \n",
    "        # Enhanced interpolation strategy\n",
    "        numeric_cols = ['y', 'qty', 'margin', 'gross_sales', 'discount']\n",
    "        \n",
    "        for col in numeric_cols:\n",
    "            if col in merged.columns:\n",
    "                if missing_ratio < 0.2:\n",
    "                    merged[col] = merged[col].interpolate(method='linear')\n",
    "                elif missing_ratio < 0.4:\n",
    "                    merged[col] = merged[col].interpolate(method='linear', limit=5)\n",
    "                else:\n",
    "                    merged[col] = merged[col].interpolate(method='linear', limit=2)\n",
    "                \n",
    "                # Forward/backward fill for remaining gaps, then zero\n",
    "                merged[col] = merged[col].fillna(method='ffill', limit=3)\n",
    "                merged[col] = merged[col].fillna(method='bfill', limit=3)\n",
    "                merged[col] = merged[col].fillna(0)\n",
    "                merged[col] = merged[col].clip(lower=0)\n",
    "        \n",
    "        # Handle exogenous variables (forward fill since they're categorical/binary)\n",
    "        exog_cols = [col for col in merged.columns if col not in ['ds', 'unique_id'] + numeric_cols]\n",
    "        for col in exog_cols:\n",
    "            if col in merged.columns:\n",
    "                merged[col] = merged[col].fillna(method='ffill')\n",
    "                merged[col] = merged[col].fillna(method='bfill')\n",
    "                merged[col] = merged[col].fillna(0)\n",
    "        \n",
    "        # Verify frequency can be inferred\n",
    "        freq_check = pd.infer_freq(merged['ds'])\n",
    "        if freq_check == 'D':\n",
    "            cleaned_data.append(merged)\n",
    "            successful_products.append(product_id)\n",
    "            print(f\"        ✅ Success: {missing_points} gaps filled\")\n",
    "        else:\n",
    "            print(f\"        ❌ Frequency check failed\")\n",
    "    \n",
    "    if cleaned_data:\n",
    "        final_data = pd.concat(cleaned_data, ignore_index=True)\n",
    "        print(f\"   ✅ Robust processing complete: {len(successful_products)} products\")\n",
    "        print(f\"   📊 Final dataset: {len(final_data):,} rows\")\n",
    "        return final_data\n",
    "    else:\n",
    "        raise ValueError(\"No products survived robust frequency correction\")\n",
    "\n",
    "# ========================================================================\n",
    "# STEP 4: TRAIN-TEST SPLIT\n",
    "# ========================================================================\n",
    "\n",
    "def create_train_test_split(data, test_days=21):\n",
    "    \"\"\"Create train-test split\"\"\"\n",
    "    print(f\"   • Splitting data with {test_days} test days...\")\n",
    "    \n",
    "    # Split data by taking last N days for testing\n",
    "    test_data = data.groupby(\"unique_id\").tail(test_days)\n",
    "    train_data = data.groupby(\"unique_id\").apply(lambda group: group.iloc[:-test_days]).reset_index(drop=True)\n",
    "    \n",
    "    print(f\"   ✅ Train: {len(train_data)} rows, Test: {len(test_data)} rows\")\n",
    "    \n",
    "    return train_data, test_data\n",
    "\n",
    "# ========================================================================\n",
    "# STEP 5: FUTURE EXOGENOUS DATA CREATION\n",
    "# ========================================================================\n",
    "\n",
    "def create_future_exogenous_data(train_data, exog_features, horizon):\n",
    "    \"\"\"Create future exogenous variable values for forecasting\"\"\"\n",
    "    future_data = []\n",
    "    \n",
    "    for unique_id in train_data['unique_id'].unique():\n",
    "        # Get last date for this product\n",
    "        product_data = train_data[train_data['unique_id'] == unique_id]\n",
    "        last_date = product_data['ds'].max()\n",
    "        \n",
    "        # Generate future dates\n",
    "        future_dates = pd.date_range(\n",
    "            start=last_date + pd.Timedelta(days=1),\n",
    "            periods=horizon,\n",
    "            freq='D'\n",
    "        )\n",
    "        \n",
    "        for date in future_dates:\n",
    "            row = {'unique_id': unique_id, 'ds': date}\n",
    "            \n",
    "            # Add predictable calendar features\n",
    "            if 'year' in exog_features:\n",
    "                row['year'] = date.year\n",
    "            if 'month' in exog_features:\n",
    "                row['month'] = date.month\n",
    "            if 'day_of_week' in exog_features:\n",
    "                row['day_of_week'] = date.dayofweek\n",
    "            if 'is_weekend' in exog_features:\n",
    "                row['is_weekend'] = int(date.dayofweek >= 5)\n",
    "            if 'is_month_start' in exog_features:\n",
    "                row['is_month_start'] = int(date.is_month_start)\n",
    "            if 'is_month_end' in exog_features:\n",
    "                row['is_month_end'] = int(date.is_month_end)\n",
    "            if 'is_quarter_start' in exog_features:\n",
    "                row['is_quarter_start'] = int(date.is_quarter_start)\n",
    "            if 'is_quarter_end' in exog_features:\n",
    "                row['is_quarter_end'] = int(date.is_quarter_end)\n",
    "            if 'quarter' in exog_features:\n",
    "                row['quarter'] = date.quarter\n",
    "            \n",
    "            # Add holiday indicators\n",
    "            if 'is_republic_day' in exog_features:\n",
    "                row['is_republic_day'] = int(date.strftime('%m-%d') == '01-26')\n",
    "            if 'is_independence_day' in exog_features:\n",
    "                row['is_independence_day'] = int(date.strftime('%m-%d') == '08-15')\n",
    "            if 'is_christmas' in exog_features:\n",
    "                row['is_christmas'] = int(date.strftime('%m-%d') == '12-25')\n",
    "            \n",
    "            # For variable date holidays, set to 0 for now (can be enhanced later)\n",
    "            for holiday in ['is_holi', 'is_diwali']:\n",
    "                if holiday in exog_features:\n",
    "                    row[holiday] = 0\n",
    "            \n",
    "            future_data.append(row)\n",
    "    \n",
    "    return pd.DataFrame(future_data)\n",
    "\n",
    "# ========================================================================\n",
    "# STEP 6: TIMEGPT FORECASTING\n",
    "# ========================================================================\n",
    "\n",
    "def run_timegpt_forecasting_with_exogenous(train_data, predictable_features, test_periods=7):\n",
    "    \"\"\"TimeGPT forecasting with proper exogenous variable handling\"\"\"\n",
    "    print(f\"   🎯 Running TimeGPT forecast for {test_periods} periods...\")\n",
    "    \n",
    "    try:\n",
    "        # Check available exogenous features\n",
    "        available_exog = [f for f in predictable_features if f in train_data.columns]\n",
    "        \n",
    "        if available_exog:\n",
    "            print(f\"   • Using exogenous variables: {available_exog}\")\n",
    "            \n",
    "            # Create future exogenous data for the test period\n",
    "            future_X_df = create_future_exogenous_data(train_data, available_exog, test_periods)\n",
    "            \n",
    "            # Main dataframe with target and historical exogenous variables\n",
    "            main_data = train_data[['unique_id', 'ds', 'y'] + available_exog].copy()\n",
    "            \n",
    "            print(f\"   • Main data shape: {main_data.shape}\")\n",
    "            print(f\"   • Future exogenous data shape: {future_X_df.shape}\")\n",
    "            \n",
    "            # Generate forecasts with exogenous variables\n",
    "            forecasts = timegpt.forecast(\n",
    "                df=main_data,          # Historical data with exogenous variables\n",
    "                X_df=future_X_df,      # Future exogenous variables\n",
    "                h=test_periods,\n",
    "                freq='D',\n",
    "                level=[80, 90],\n",
    "                time_col='ds',\n",
    "                target_col='y',\n",
    "                id_col='unique_id',\n",
    "                finetune_steps=30,\n",
    "                finetune_loss='mae',\n",
    "                clean_ex_first=True\n",
    "            )\n",
    "            \n",
    "        else:\n",
    "            print(\"   • No exogenous variables available, using target only\")\n",
    "            main_data = train_data[['unique_id', 'ds', 'y']].copy()\n",
    "            \n",
    "            forecasts = timegpt.forecast(\n",
    "                df=main_data,\n",
    "                h=test_periods,\n",
    "                freq='D',\n",
    "                level=[80, 90],\n",
    "                time_col='ds',\n",
    "                target_col='y',\n",
    "                id_col='unique_id',\n",
    "                finetune_steps=30,\n",
    "                finetune_loss='mae'\n",
    "            )\n",
    "        \n",
    "        print(f\"   ✅ Forecasts generated: {len(forecasts):,} rows\")\n",
    "        return forecasts\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"   ❌ Forecasting failed: {e}\")\n",
    "        print(\"   🔄 Trying basic approach without exogenous variables...\")\n",
    "        \n",
    "        # Fallback: Use only target variables\n",
    "        try:\n",
    "            basic_data = train_data[['unique_id', 'ds', 'y']].copy()\n",
    "            \n",
    "            forecasts = timegpt.forecast(\n",
    "                df=basic_data,\n",
    "                h=test_periods,\n",
    "                freq='D',\n",
    "                level=[80, 90],\n",
    "                time_col='ds',\n",
    "                target_col='y',\n",
    "                id_col='unique_id',\n",
    "                finetune_steps=20,\n",
    "                finetune_loss='mae'\n",
    "            )\n",
    "            \n",
    "            print(f\"   ✅ Basic forecasts generated: {len(forecasts):,} rows\")\n",
    "            return forecasts\n",
    "            \n",
    "        except Exception as e2:\n",
    "            print(f\"   ❌ All approaches failed: {e2}\")\n",
    "            return None\n",
    "\n",
    "# ========================================================================\n",
    "# STEP 7: EVALUATION\n",
    "# ========================================================================\n",
    "\n",
    "def evaluate_forecasts(forecasts, test_data):\n",
    "    \"\"\"Evaluate forecast performance with improved MAPE handling\"\"\"\n",
    "    if forecasts is None:\n",
    "        return None\n",
    "        \n",
    "    print(\"   • Calculating performance metrics...\")\n",
    "    \n",
    "    # Fix datetime types\n",
    "    forecasts['ds'] = pd.to_datetime(forecasts['ds'])\n",
    "    test_data['ds'] = pd.to_datetime(test_data['ds'])\n",
    "    \n",
    "    # Merge forecasts with test data\n",
    "    evaluation_data = pd.merge(\n",
    "        test_data,\n",
    "        forecasts[['ds', 'unique_id', 'TimeGPT']],\n",
    "        on=['ds', 'unique_id'],\n",
    "        how='inner'\n",
    "    )\n",
    "    \n",
    "    if len(evaluation_data) == 0:\n",
    "        print(\"   ❌ No matching data for evaluation\")\n",
    "        return None\n",
    "    \n",
    "    # Calculate metrics by product\n",
    "    product_metrics = []\n",
    "    \n",
    "    for product_id in evaluation_data['unique_id'].unique():\n",
    "        product_eval = evaluation_data[evaluation_data['unique_id'] == product_id]\n",
    "        \n",
    "        if len(product_eval) > 0:\n",
    "            # Standard metrics\n",
    "            mse = mean_squared_error(product_eval['y'], product_eval['TimeGPT'])\n",
    "            mae = mean_absolute_error(product_eval['y'], product_eval['TimeGPT'])\n",
    "            rmse = np.sqrt(mse)\n",
    "            \n",
    "            # Improved MAPE calculation with zero handling\n",
    "            actual_values = product_eval['y'].values\n",
    "            predicted_values = product_eval['TimeGPT'].values\n",
    "            \n",
    "            # Exclude zero values from MAPE calculation\n",
    "            non_zero_mask = actual_values != 0\n",
    "            if non_zero_mask.sum() > 0:\n",
    "                mape = np.mean(np.abs((actual_values[non_zero_mask] - predicted_values[non_zero_mask]) / actual_values[non_zero_mask])) * 100\n",
    "                effective_points = non_zero_mask.sum()\n",
    "            else:\n",
    "                mape = np.inf\n",
    "                effective_points = 0\n",
    "            \n",
    "            # sMAPE (Symmetric MAPE) - more robust to zeros\n",
    "            smape = np.mean(2 * np.abs(actual_values - predicted_values) / \n",
    "                           (np.abs(actual_values) + np.abs(predicted_values) + 1e-8)) * 100\n",
    "            \n",
    "            # WAPE (Weighted Absolute Percentage Error)\n",
    "            if actual_values.sum() != 0:\n",
    "                wape = np.sum(np.abs(actual_values - predicted_values)) / np.sum(np.abs(actual_values)) * 100\n",
    "            else:\n",
    "                wape = np.inf\n",
    "            \n",
    "            product_metrics.append({\n",
    "                'product': product_id,\n",
    "                'mse': mse,\n",
    "                'mae': mae,\n",
    "                'rmse': rmse,\n",
    "                'mape': mape,\n",
    "                'smape': smape,\n",
    "                'wape': wape,\n",
    "                'data_points': len(product_eval),\n",
    "                'effective_points': effective_points,\n",
    "                'zero_values': (actual_values == 0).sum()\n",
    "            })\n",
    "    \n",
    "    metrics_df = pd.DataFrame(product_metrics)\n",
    "    \n",
    "    # Calculate overall statistics\n",
    "    finite_mape_mask = np.isfinite(metrics_df['mape'])\n",
    "    \n",
    "    if finite_mape_mask.sum() > 0:\n",
    "        avg_mape = metrics_df.loc[finite_mape_mask, 'mape'].mean()\n",
    "        mape_note = f\"(calculated from {finite_mape_mask.sum()}/{len(metrics_df)} products)\"\n",
    "    else:\n",
    "        avg_mape = np.inf\n",
    "        mape_note = \"(all products have zero values - use sMAPE instead)\"\n",
    "    \n",
    "    overall_stats = {\n",
    "        'avg_mse': metrics_df['mse'].mean(),\n",
    "        'avg_mae': metrics_df['mae'].mean(),\n",
    "        'avg_rmse': metrics_df['rmse'].mean(),\n",
    "        'avg_mape': avg_mape,\n",
    "        'avg_smape': metrics_df['smape'].mean(),\n",
    "        'avg_wape': metrics_df[np.isfinite(metrics_df['wape'])]['wape'].mean(),\n",
    "        'total_products': len(metrics_df),\n",
    "        'total_points': len(evaluation_data),\n",
    "        'products_with_zeros': (metrics_df['zero_values'] > 0).sum()\n",
    "    }\n",
    "    \n",
    "    print(f\"   ✅ Evaluation complete: {len(metrics_df)} products evaluated\")\n",
    "    print(f\"   📊 Average MAE: {overall_stats['avg_mae']:.2f}\")\n",
    "    print(f\"   📊 Average RMSE: {overall_stats['avg_rmse']:.2f}\")\n",
    "    print(f\"   📊 Average sMAPE: {overall_stats['avg_smape']:.2f}% (recommended)\")\n",
    "    print(f\"   📊 Average MAPE: {overall_stats['avg_mape']:.2f}% {mape_note}\")\n",
    "    \n",
    "    return {\n",
    "        'product_metrics': metrics_df,\n",
    "        'overall_stats': overall_stats,\n",
    "        'evaluation_data': evaluation_data\n",
    "    }\n",
    "\n",
    "# ========================================================================\n",
    "# STEP 8: VISUALIZATION\n",
    "# ========================================================================\n",
    "\n",
    "def create_comprehensive_visualizations(full_data, forecasts, test_data, evaluation_results):\n",
    "    \"\"\"Create comprehensive visualizations\"\"\"\n",
    "    print(\"   • Creating visualizations...\")\n",
    "    \n",
    "    if evaluation_results is None or forecasts is None:\n",
    "        print(\"   ❌ No evaluation data for visualization\")\n",
    "        return\n",
    "    \n",
    "    try:\n",
    "        # Use TimeGPT's built-in plotting function\n",
    "        timegpt.plot(\n",
    "            test_data,\n",
    "            forecasts,\n",
    "            models=[\"TimeGPT\"],\n",
    "            level=[90],\n",
    "            time_col=\"ds\",\n",
    "            target_col=\"y\",\n",
    "            id_col=\"unique_id\",\n",
    "            max_insample_length=60\n",
    "        )\n",
    "        \n",
    "        print(\"   ✅ TimeGPT native plot created\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"   ⚠️ TimeGPT plot failed: {e}\")\n",
    "        print(\"   🔄 Creating custom matplotlib visualization...\")\n",
    "        \n",
    "        # Fallback to custom visualization\n",
    "        evaluation_data = evaluation_results['evaluation_data']\n",
    "        product_metrics = evaluation_results['product_metrics']\n",
    "        \n",
    "        # Get top 3 products for plotting\n",
    "        top_products = product_metrics.nsmallest(3, 'smape')['product'].tolist()\n",
    "        \n",
    "        fig, axes = plt.subplots(len(top_products), 1, figsize=(15, 5*len(top_products)))\n",
    "        if len(top_products) == 1:\n",
    "            axes = [axes]\n",
    "        \n",
    "        for idx, product_id in enumerate(top_products):\n",
    "            # Get data for this product\n",
    "            product_train = full_data[full_data['unique_id'] == product_id]\n",
    "            product_test = evaluation_data[evaluation_data['unique_id'] == product_id]\n",
    "            product_pred = forecasts[forecasts['unique_id'] == product_id]\n",
    "            \n",
    "            # Get metrics\n",
    "            metrics = product_metrics[product_metrics['product'] == product_id].iloc[0]\n",
    "            \n",
    "            # Plot training data (last 30 days)\n",
    "            if len(product_test) > 0:\n",
    "                test_start = product_test['ds'].min()\n",
    "                train_plot = product_train[product_train['ds'] >= test_start - pd.Timedelta(days=30)]\n",
    "                \n",
    "                axes[idx].plot(train_plot['ds'], train_plot['y'], 'b-', linewidth=2, label='Training Data', alpha=0.7)\n",
    "                axes[idx].plot(product_test['ds'], product_test['y'], 'go-', linewidth=3, markersize=8, label='Actual Test Data')\n",
    "                axes[idx].plot(product_pred['ds'], product_pred['TimeGPT'], 'r--', linewidth=3, marker='s', markersize=8, label='TimeGPT Predictions')\n",
    "                \n",
    "                # Add confidence intervals if available\n",
    "                if 'TimeGPT-lo-90' in product_pred.columns:\n",
    "                    axes[idx].fill_between(product_pred['ds'], product_pred['TimeGPT-lo-90'], product_pred['TimeGPT-hi-90'], color='red', alpha=0.2, label='90% Confidence')\n",
    "                \n",
    "                # Formatting\n",
    "                title = f'Product: {product_id[:50]}...\\n'\n",
    "                title += f'sMAPE: {metrics[\"smape\"]:.1f}% | MAE: {metrics[\"mae\"]:.1f}'\n",
    "                \n",
    "                axes[idx].set_title(title, fontsize=12, fontweight='bold')\n",
    "                axes[idx].set_xlabel('Date')\n",
    "                axes[idx].set_ylabel('Sales')\n",
    "                axes[idx].legend()\n",
    "                axes[idx].grid(True, alpha=0.3)\n",
    "                \n",
    "                # Format dates\n",
    "                axes[idx].xaxis.set_major_formatter(mdates.DateFormatter('%m/%d'))\n",
    "                plt.setp(axes[idx].xaxis.get_majorticklabels(), rotation=45)\n",
    "        \n",
    "        plt.suptitle('🤖 TimeGPT Forecasting Results', fontsize=16, fontweight='bold')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        print(\"   ✅ Custom visualization created\")\n",
    "\n",
    "# ========================================================================\n",
    "# STEP 9: FUTURE FORECASTING\n",
    "# ========================================================================\n",
    "\n",
    "def generate_future_forecasts(data, predictable_features, horizon=14):\n",
    "    \"\"\"Generate future forecasts with proper exogenous variable handling\"\"\"\n",
    "    print(f\"   • Generating {horizon}-day future forecasts...\")\n",
    "    \n",
    "    try:\n",
    "        # Check if exogenous variables exist\n",
    "        available_exog = [col for col in predictable_features if col in data.columns]\n",
    "        \n",
    "        if available_exog:\n",
    "            print(f\"   • Including exogenous features: {available_exog}\")\n",
    "            \n",
    "            # Create future exogenous data\n",
    "            future_X_df = create_future_exogenous_data(data, available_exog, horizon)\n",
    "            \n",
    "            # Main data for forecasting\n",
    "            main_data = data[['unique_id', 'ds', 'y'] + available_exog].copy()\n",
    "            \n",
    "            future_forecasts = timegpt.forecast(\n",
    "                df=main_data,\n",
    "                X_df=future_X_df,\n",
    "                h=horizon,\n",
    "                freq='D',\n",
    "                level=[80, 90],\n",
    "                time_col='ds',\n",
    "                target_col='y',\n",
    "                id_col='unique_id'\n",
    "            )\n",
    "        else:\n",
    "            # No exogenous variables\n",
    "            future_forecasts = timegpt.forecast(\n",
    "                df=data[['unique_id', 'ds', 'y']],\n",
    "                h=horizon,\n",
    "                freq='D',\n",
    "                level=[80, 90],\n",
    "                time_col='ds',\n",
    "                target_col='y',\n",
    "                id_col='unique_id'\n",
    "            )\n",
    "        \n",
    "        # Create summary\n",
    "        forecast_summary = future_forecasts.groupby('unique_id').agg({\n",
    "            'TimeGPT': ['mean', 'sum']\n",
    "        }).round(2)\n",
    "        \n",
    "        forecast_summary.columns = ['avg_daily', 'total_forecast']\n",
    "        forecast_summary = forecast_summary.sort_values('total_forecast', ascending=False)\n",
    "        \n",
    "        print(f\"   ✅ Future forecasts generated\")\n",
    "        print(f\"   📊 Top 5 products by projected sales:\")\n",
    "        print(forecast_summary.head())\n",
    "        \n",
    "        return future_forecasts\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"   ❌ Future forecast error: {e}\")\n",
    "        return None\n",
    "\n",
    "# ========================================================================\n",
    "# MAIN PIPELINE FUNCTION\n",
    "# ========================================================================\n",
    "\n",
    "def complete_timegpt_pipeline_with_exogenous(df_sorted, top_n_products=8):\n",
    "    \"\"\"Complete end-to-end TimeGPT forecasting pipeline with exogenous features\"\"\"\n",
    "    print(\"🚀 STARTING COMPLETE TIMEGPT PIPELINE WITH EXOGENOUS FEATURES\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    # Step 1: Enhanced Data Cleaning and Preparation with Exogenous Features\n",
    "    print(\"\\n📋 STEP 1: DATA CLEANING WITH EXOGENOUS FEATURES\")\n",
    "    cleaned_data, predictable_features = clean_and_prepare_data_with_exogenous(df_sorted, top_n_products)\n",
    "    \n",
    "    # Step 2: Fix Frequency Issues\n",
    "    print(\"\\n🔧 STEP 2: FIXING FREQUENCY ISSUES\")\n",
    "    frequency_fixed_data = fix_frequency_issues_complete(cleaned_data)\n",
    "    \n",
    "    # Step 3: Train-Test Split\n",
    "    print(\"\\n✂️ STEP 3: TRAIN-TEST SPLIT\")\n",
    "    train_data, test_data = create_train_test_split(frequency_fixed_data)\n",
    "    \n",
    "    # Step 4: TimeGPT Forecasting with Exogenous Features\n",
    "    print(\"\\n🤖 STEP 4: TIMEGPT FORECASTING WITH EXOGENOUS FEATURES\")\n",
    "    forecasts = run_timegpt_forecasting_with_exogenous(train_data, predictable_features, test_periods=7)\n",
    "    \n",
    "    # Step 5: Evaluation\n",
    "    print(\"\\n📊 STEP 5: MODEL EVALUATION\")\n",
    "    evaluation_results = evaluate_forecasts(forecasts, test_data)\n",
    "    \n",
    "    # Step 6: Visualization\n",
    "    print(\"\\n📈 STEP 6: CREATING VISUALIZATIONS\")\n",
    "    create_comprehensive_visualizations(frequency_fixed_data, forecasts, test_data, evaluation_results)\n",
    "    \n",
    "    # Step 7: Future Forecasting\n",
    "    print(\"\\n🔮 STEP 7: FUTURE FORECASTING\")\n",
    "    future_forecasts = generate_future_forecasts(frequency_fixed_data, predictable_features, horizon=14)\n",
    "    \n",
    "    return {\n",
    "        'cleaned_data': frequency_fixed_data,\n",
    "        'train_data': train_data,\n",
    "        'test_data': test_data,\n",
    "        'forecasts': forecasts,\n",
    "        'evaluation': evaluation_results,\n",
    "        'future_forecasts': future_forecasts,\n",
    "        'exogenous_features': predictable_features\n",
    "    }\n",
    "\n",
    "# ========================================================================\n",
    "# EXECUTION\n",
    "# ========================================================================\n",
    "\n",
    "# Load your data first\n",
    "# df_sorted should be your sorted dataframe with BILLING_DATE sorted\n",
    "\n",
    "# Run the complete pipeline\n",
    "results = complete_timegpt_pipeline_with_exogenous(df_sorted, top_n_products=8)\n",
    "\n",
    "print(\"\\n🎉 PIPELINE COMPLETED SUCCESSFULLY!\")\n",
    "print(\"=\" * 70)\n",
    "print(\"📊 RESULTS SUMMARY:\")\n",
    "if results['evaluation']:\n",
    "    stats = results['evaluation']['overall_stats']\n",
    "    print(f\"• Products forecasted: {stats['total_products']}\")\n",
    "    print(f\"• Average MAE: {stats['avg_mae']:.2f}\")\n",
    "    print(f\"• Average sMAPE: {stats['avg_smape']:.2f}%\")\n",
    "print(\"• Check the visualizations above for detailed results\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "agent_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
